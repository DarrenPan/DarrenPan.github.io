<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Good Good Study">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Good Good Study">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Good Good Study">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Good Good Study</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Good Good Study</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/03/leetcode_python_medium/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/12/03/leetcode_python_medium/" itemprop="url">Leetcode_Python_Medium</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-12-03T14:17:48+08:00">
                2019-12-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="2-Add-Two-Numbers"><a href="#2-Add-Two-Numbers" class="headerlink" title="2. Add Two Numbers"></a>2. Add Two Numbers</h1><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给出两个 非空 的链表( linked lists)用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。</p>
<p>如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。</p>
<p>您可以假设除了数字 0 之外，这两个数都不会以 0 开头。</p>
<h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)</span><br><span class="line">输出：7 -&gt; 0 -&gt; 8</span><br><span class="line">原因：342 + 465 = 807</span><br></pre></td></tr></table></figure>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">         self.val = x</span><br><span class="line">         self.next = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addTwoNumbers</span><span class="params">(self, l1, l2)</span>:</span></span><br><span class="line">    <span class="comment"># def addTwoNumbers(self, l1: ListNode, l2: ListNode) -&gt; ListNode:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type l1: ListNode</span></span><br><span class="line"><span class="string">        :type l2: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        head = curr = ListNode(<span class="number">0</span>)</span><br><span class="line">        carry = <span class="number">0</span> <span class="comment"># 进位</span></span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">or</span> l2:</span><br><span class="line">            <span class="comment"># 从低到高 逐位相加</span></span><br><span class="line">            sum = carry</span><br><span class="line">            <span class="keyword">if</span> l1:</span><br><span class="line">                sum += l1.val</span><br><span class="line">                l1 = l1.next</span><br><span class="line">            <span class="keyword">if</span> l2:</span><br><span class="line">                sum += l2.val</span><br><span class="line">                l2 = l2.next</span><br><span class="line">            curr.next = ListNode(sum%<span class="number">10</span>)  <span class="comment"># 个位保留 十位做进位</span></span><br><span class="line">            curr = curr.next</span><br><span class="line">            carry = sum / <span class="number">10</span></span><br><span class="line">        <span class="keyword">if</span> carry &gt; <span class="number">0</span>: <span class="comment"># 特别注意 两个链表都加完后是否还有进位</span></span><br><span class="line">            curr.next = ListNode(carry)</span><br><span class="line">        <span class="keyword">return</span> head.next</span><br></pre></td></tr></table></figure>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>1.Python2用/ Python3用 // 表示下取整的除法<br>2.链表的构建<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">head = curr = ListNode(0)</span><br><span class="line">...</span><br><span class="line">curr.next = ListNode(XX)</span><br><span class="line">curr = curr.next</span><br><span class="line">...</span><br><span class="line">return head.next</span><br></pre></td></tr></table></figure></p>
<p>3.生成链表的方法及本题测试代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateList</span><span class="params">(l: list)</span> -&gt; ListNode:</span></span><br><span class="line">    prenode = ListNode(<span class="number">0</span>)</span><br><span class="line">    lastnode = prenode</span><br><span class="line">    <span class="keyword">for</span> val <span class="keyword">in</span> l:</span><br><span class="line">        lastnode.next = ListNode(val)</span><br><span class="line">        lastnode = lastnode.next</span><br><span class="line">    <span class="keyword">return</span> prenode.next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printList</span><span class="params">(l: ListNode)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> l:</span><br><span class="line">        print(<span class="string">"%d, "</span> %(l.val), end = <span class="string">''</span>)</span><br><span class="line">        l = l.next</span><br><span class="line">    print(<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    l1 = generateList([<span class="number">1</span>, <span class="number">5</span>, <span class="number">8</span>])</span><br><span class="line">    l2 = generateList([<span class="number">9</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">9</span>])</span><br><span class="line">    printList(l1)</span><br><span class="line">    printList(l2)</span><br><span class="line">    s = Solution()</span><br><span class="line">    sum = s.addTwoNumbers(l1, l2)</span><br><span class="line">    printList(sum)</span><br></pre></td></tr></table></figure>
<h1 id="3-Longest-Substring-Without-Repeating-Characters"><a href="#3-Longest-Substring-Without-Repeating-Characters" class="headerlink" title="3. Longest Substring Without Repeating Characters"></a>3. Longest Substring Without Repeating Characters</h1><h2 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。。</p>
<h2 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">输入: &quot;abcabcbb&quot;</span><br><span class="line">输出: 3 </span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。</span><br><span class="line"></span><br><span class="line">输入: &quot;bbbbb&quot;</span><br><span class="line">输出: 1</span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。</span><br><span class="line"></span><br><span class="line">输入: &quot;pwwkew&quot;</span><br><span class="line">输出: 3</span><br><span class="line">解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。</span><br><span class="line">     请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。</span><br></pre></td></tr></table></figure>
<h2 id="Solution1"><a href="#Solution1" class="headerlink" title="Solution1"></a>Solution1</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        max_len = <span class="number">0</span> <span class="comment"># 储存最大长度</span></span><br><span class="line">        start = <span class="number">0</span> <span class="comment"># 记录子串开始的index</span></span><br><span class="line">        substring = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> index, c <span class="keyword">in</span> enumerate(s):</span><br><span class="line">            <span class="comment"># 不断记录下子串，直到出现重复字符，记录下原子串的长度，</span></span><br><span class="line">            <span class="comment"># 并丢弃原子串中该字符及之前的字符（更新start位置），将新的字符加入子串中</span></span><br><span class="line">            <span class="keyword">if</span> c <span class="keyword">in</span> substring <span class="keyword">and</span> substring[c] &gt;= start:</span><br><span class="line">                max_len = max(max_len, index-start)</span><br><span class="line">                start = substring[c] + <span class="number">1</span></span><br><span class="line">            substring[c] = index</span><br><span class="line">        <span class="keyword">return</span> max(max_len, len(s) - start)  <span class="comment">#遍历到最后的一串无重复字符子串长度为len(s)-start</span></span><br></pre></td></tr></table></figure>
<h2 id="Solution2"><a href="#Solution2" class="headerlink" title="Solution2"></a>Solution2</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        substring = <span class="string">""</span></span><br><span class="line">        max_len = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> s:  </span><br><span class="line">            index = substring.find(c)  <span class="comment"># 直接对字符串操作</span></span><br><span class="line">            <span class="keyword">if</span> index == <span class="number">-1</span>:</span><br><span class="line">                substring += c</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                max_len = max(max_len, len(substring))         </span><br><span class="line">                substring = substring[index+<span class="number">1</span>:] + c  <span class="comment">#注意这里不是[index+1:-1]</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> max(max_len, len(substring))</span><br></pre></td></tr></table></figure>
<h1 id="5-Longest-Palindromic-回文-Substring-！！！！！！！！！！！！！！！！"><a href="#5-Longest-Palindromic-回文-Substring-！！！！！！！！！！！！！！！！" class="headerlink" title="5. Longest Palindromic(回文) Substring ！！！！！！！！！！！！！！！！"></a>5. Longest Palindromic(回文) Substring ！！！！！！！！！！！！！！！！</h1><h2 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。</p>
<h2 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">输入: &quot;babad&quot;</span><br><span class="line">输出: &quot;bab&quot;</span><br><span class="line">注意: &quot;aba&quot; 也是一个有效答案。</span><br><span class="line"></span><br><span class="line">输入: &quot;cbbd&quot;</span><br><span class="line">输出: &quot;bb&quot;</span><br></pre></td></tr></table></figure>
<h2 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h2><h1 id="6-ZigZag-Conversion-Z字形变换"><a href="#6-ZigZag-Conversion-Z字形变换" class="headerlink" title="6. ZigZag Conversion (Z字形变换)"></a>6. ZigZag Conversion (Z字形变换)</h1><p><em>相关标签</em>: <code>字符串</code></p>
<h2 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h2><p>将一个给定字符串根据给定的行数，以从上往下、从左到右进行 Z 字形排列。</p>
<p>比如输入字符串为 “LEETCODEISHIRING” 行数为 3 时，排列如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">L   C   I   R</span><br><span class="line">E T O E S I I G</span><br><span class="line">E   D   H   N</span><br></pre></td></tr></table></figure></p>
<p>之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：”LCIRETOESIIGEDHN”。</p>
<p>请你实现这个将字符串进行指定行数变换的函数 <code>string convert(string s, int numRows);</code></p>
<h2 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">输入: s = &quot;LEETCODEISHIRING&quot;, numRows = 3</span><br><span class="line">输出: &quot;LCIRETOESIIGEDHN&quot;</span><br><span class="line"></span><br><span class="line">输入: s = &quot;LEETCODEISHIRING&quot;, numRows = 4</span><br><span class="line">输出: &quot;LDREOEIIECIHNTSG&quot;</span><br><span class="line">解释:</span><br><span class="line"></span><br><span class="line">L     D     R</span><br><span class="line">E   O E   I I</span><br><span class="line">E C   I H   N</span><br><span class="line">T     S     G</span><br></pre></td></tr></table></figure>
<h2 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(self, s: str, numRows: int)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">if</span> numRows &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line">        row = [<span class="string">""</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(numRows)]</span><br><span class="line">        row_id = <span class="number">0</span></span><br><span class="line">        flag = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> s:</span><br><span class="line">            row[row_id] += c</span><br><span class="line">            <span class="keyword">if</span> row_id==<span class="number">0</span> <span class="keyword">or</span> row_id == numRows - <span class="number">1</span>:</span><br><span class="line">                flag = -flag</span><br><span class="line">            row_id += flag</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(row)</span><br></pre></td></tr></table></figure>
<h1 id="11-Container-With-Most-Water"><a href="#11-Container-With-Most-Water" class="headerlink" title="11. Container With Most Water"></a>11. Container With Most Water</h1><p><em>相关标签</em>: <code>双指针</code></p>
<h2 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定 n 个非负整数 a1，a2，…，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水</p>
<h2 id="示例-4"><a href="#示例-4" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: [1,8,6,2,5,4,8,3,7]</span><br><span class="line">输出: 49</span><br></pre></td></tr></table></figure>
<h2 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxArea</span><span class="params">(self, height: List[int])</span> -&gt; int:</span></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        j = len(height) - <span class="number">1</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; j:</span><br><span class="line">            <span class="keyword">if</span> height[i] &lt; height[j]:</span><br><span class="line">                res = max(res, height[i] * (j - i))</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res = max(res, height[j] * (j - i))</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p><strong>算法流程</strong>： 设置双指针 i,j 分别位于容器壁两端，根据规则移动指针（后续说明），并且更新面积最大值 res，直到 i == j 时返回 res。</p>
<p><strong>指针移动规则与证明</strong>： 每次选定围成水槽两板高度 h[i],h[j]中的短板，向中间收窄 111 格。以下证明：</p>
<ul>
<li>设每一状态下水槽面积为 S(i,j),(0&lt;=i&lt;j&lt;n)，由于水槽的实际高度由两板中的短板决定，则可得面积公式 S(i,j)=min(h[i],h[j])×(j−i)S(i, j)。</li>
<li>在每一个状态下，无论长板或短板收窄 1格，都会导致水槽 底边宽度 −1。</li>
<li>若向内移动短板，水槽的短板 min(h[i],h[j])可能变大，因此水槽面积 S(i,j)S(i, j)S(i,j) 可能增大。</li>
<li>若向内移动长板，水槽的短板 min(h[i],h[j])不变或变小，下个水槽的面积一定小于当前水槽面积。</li>
</ul>
<h1 id="12-Integer-to-Roman"><a href="#12-Integer-to-Roman" class="headerlink" title="12. Integer to Roman"></a>12. Integer to Roman</h1><p><em>相关标签</em>: <code>字符串</code></p>
<h2 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h2><p> 罗马数字包含以下七种字符： <code>I</code>， <code>V</code>， <code>X</code>， <code>L</code>，<code>C</code>，<code>D</code> 和 <code>M</code>。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">字符          数值</span><br><span class="line">I             1</span><br><span class="line">V             5</span><br><span class="line">X             10</span><br><span class="line">L             50</span><br><span class="line">C             100</span><br><span class="line">D             500</span><br><span class="line">M             1000</span><br></pre></td></tr></table></figure></p>
<p>例如， 罗马数字 2 写做 II ，即为两个并列的 I。12 写做 XII ，即为 X + II 。 27 写做  XXVII, 即为 XX + V + II 。</p>
<p>通常情况下，罗马数字中小的数字在大的数字的右边。但也存在特例，例如 4 不写做 IIII，而是 IV。数字 1 在数字 5 的左边，所表示的数等于大数 5 减小数 1 得到的数值 4 。同样地，数字 9 表示为 IX。这个特殊的规则只适用于以下六种情况：</p>
<pre><code>I 可以放在 V (5) 和 X (10) 的左边，来表示 4 和 9。
X 可以放在 L (50) 和 C (100) 的左边，来表示 40 和 90。 
C 可以放在 D (500) 和 M (1000) 的左边，来表示 400 和 900。
</code></pre><p>给定一个整数，将其转为罗马数字。输入确保在 1 到 3999 的范围内。</p>
<h2 id="示例-5"><a href="#示例-5" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">输入: 3</span><br><span class="line">输出: &quot;III&quot;</span><br><span class="line"></span><br><span class="line">输入: 4</span><br><span class="line">输出: &quot;IV&quot;</span><br><span class="line"></span><br><span class="line">输入: 9</span><br><span class="line">输出: &quot;IX&quot;</span><br><span class="line"></span><br><span class="line">输入: 58</span><br><span class="line">输出: &quot;LVIII&quot;</span><br><span class="line"></span><br><span class="line">输入: 1994</span><br><span class="line">输出: &quot;MCMXCIV&quot;</span><br></pre></td></tr></table></figure>
<h2 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intToRoman</span><span class="params">(self, num: int)</span> -&gt; str:</span></span><br><span class="line">        res = <span class="string">""</span></span><br><span class="line">        values = [<span class="number">1000</span>, <span class="number">900</span>, <span class="number">500</span>, <span class="number">400</span>,</span><br><span class="line">                  <span class="number">100</span>, <span class="number">90</span>, <span class="number">50</span>, <span class="number">40</span>,</span><br><span class="line">                  <span class="number">10</span>, <span class="number">9</span>, <span class="number">5</span>, <span class="number">4</span>,</span><br><span class="line">                  <span class="number">1</span>]</span><br><span class="line">        symbols = [<span class="string">'M'</span>, <span class="string">'CM'</span>, <span class="string">'D'</span>, <span class="string">'CD'</span>,</span><br><span class="line">                   <span class="string">'C'</span>, <span class="string">'XC'</span>, <span class="string">'L'</span>, <span class="string">'XL'</span>,</span><br><span class="line">                   <span class="string">'X'</span>, <span class="string">'IX'</span>, <span class="string">'V'</span>, <span class="string">'IV'</span>,</span><br><span class="line">                   <span class="string">'I'</span>]</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> num &gt; <span class="number">0</span>:</span><br><span class="line">            count = num // values[i]</span><br><span class="line">            res += <span class="string">""</span>.join([symbols[i] <span class="keyword">for</span> _ <span class="keyword">in</span> range(count)])</span><br><span class="line">            num -= count * values[i]</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h1 id="15-3SUM"><a href="#15-3SUM" class="headerlink" title="15. 3SUM"></a>15. 3SUM</h1><p><em>相关标签</em>: <code>双指针</code> <code>数组</code></p>
<h2 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。</p>
<p><strong>注意</strong>：答案中不可以包含重复的三元组。</p>
<h2 id="示例-6"><a href="#示例-6" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">给定数组 nums = [-1, 0, 1, 2, -1, -4]，</span><br><span class="line"></span><br><span class="line">满足要求的三元组集合为：</span><br><span class="line">[</span><br><span class="line">  [-1, 0, 1],</span><br><span class="line">  [-1, -1, 2]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h2 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSum</span><span class="params">(self, nums: List[int])</span> -&gt; List[List[int]]:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(len(nums)<span class="number">-2</span>):</span><br><span class="line">            <span class="keyword">if</span> nums[k] &gt; <span class="number">0</span>: <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> k &gt; <span class="number">0</span> <span class="keyword">and</span> nums[k] == nums[k<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line">            i, j = k+<span class="number">1</span>, len(nums)<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> i &lt; j:</span><br><span class="line">                s = nums[k] + nums[i] + nums[j]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> s &gt; <span class="number">0</span>:</span><br><span class="line">                    j -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> i &lt; j <span class="keyword">and</span> nums[j] == nums[j+<span class="number">1</span>]: j -= <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> s &lt; <span class="number">0</span>:</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> i &lt; j <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: i += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res.append([nums[k], nums[i], nums[j]])</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">                    j -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> i &lt; j <span class="keyword">and</span> nums[j] == nums[j+<span class="number">1</span>]: j -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> i &lt; j <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><p><img src="/2019/12/03/leetcode_python_medium/1.png" alt=""></p>
<h1 id="16-3Sum-Closest"><a href="#16-3Sum-Closest" class="headerlink" title="16. 3Sum Closest"></a>16. 3Sum Closest</h1><p><em>相关标签</em>: <code>双指针</code> <code>数组</code> </p>
<h2 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个包括 n 个整数的数组 nums 和 一个目标值 target。找出 nums 中的三个整数，使得它们的和与 target 最接近。返回这三个数的和。假定每组输入只存在唯一答案。</p>
<h2 id="示例-7"><a href="#示例-7" class="headerlink" title="示例"></a>示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定数组 nums = [-1，2，1，-4], 和 target = 1.</span><br><span class="line"></span><br><span class="line">与 target 最接近的三个数的和为 2. (-1 + 2 + 1 = 2).</span><br></pre></td></tr></table></figure>
<h2 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSumClosest</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; int:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        res = float(<span class="string">"inf"</span>)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(len(nums)<span class="number">-2</span>):</span><br><span class="line">            <span class="keyword">if</span> k &gt; <span class="number">0</span> <span class="keyword">and</span> nums[k] == nums[k<span class="number">-1</span>]: <span class="keyword">continue</span></span><br><span class="line">            i, j = k+<span class="number">1</span>, len(nums)<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> i &lt; j:</span><br><span class="line">                s = nums[k] + nums[i] + nums[j]</span><br><span class="line">                <span class="keyword">if</span> s == target:</span><br><span class="line">                    <span class="keyword">return</span> s</span><br><span class="line">                <span class="keyword">elif</span> abs(s-target) &lt; abs(res-target):</span><br><span class="line">                    res = s</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> s &lt; target:</span><br><span class="line">                    i+=<span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> i &lt; j <span class="keyword">and</span> nums[i] == nums[i<span class="number">-1</span>]: i += <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> s &gt; target:</span><br><span class="line">                    j -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">while</span> i &lt; j <span class="keyword">and</span> nums[j] == nums[j+<span class="number">1</span>]: j -= <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p><img src="/2019/12/03/leetcode_python_medium/2.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/29/gan-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/29/gan-paper/" itemprop="url">gan-paper</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-29T15:22:18+08:00">
                2019-11-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/25/kaggle-solution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/25/kaggle-solution/" itemprop="url">kaggle_solution</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-25T15:12:27+08:00">
                2019-10-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="经验分享"><a href="#经验分享" class="headerlink" title="经验分享"></a>经验分享</h1><p><a href="https://zhuanlan.zhihu.com/p/93806755" target="_blank" rel="noopener">本科生晋升GM记录 &amp; Kaggle比赛进阶技巧分享</a></p>
<h1 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h1><h2 id="调参技巧"><a href="#调参技巧" class="headerlink" title="调参技巧"></a>调参技巧</h2><p>1.<strong>Adam</strong>: init_lr=5e-4(3e-4)（⭐⭐⭐⭐⭐），3e-4号称是Adam最好的初始学习率</p>
<p>2.<strong>lr schedule</strong></p>
<ul>
<li>ReduceLROnPlateau，patience=4（5），gamma=0.1，这是我常用的一套组合，并不是最好的；</li>
<li>StepLR，个人比较喜欢用这个，自己设定好在哪个epoch进行学习率的衰减，个人比较喜欢用的衰减步骤是[5e-4(3e-4), 1e-4, 1e-5, 1e-6]，至于衰减位置，就需要自己有比较好的直觉，或者就是看log调参，对着2.1上训练的valid  loss走势，valid loss不收敛了，咱就立刻进行衰减；</li>
<li>CosineAnnealingLR+Multi cycle,这个相较于前两个，就不需要太多的调参，可以训练多个cycle，模型可以找到更多的局部最优，一般推荐min_lr=1e-6，至于每个cycle多少epoch这个就说不准了，不同数据不太一样。</li>
</ul>
<p>3.<strong>finetune</strong>，微调也是有许多比较fancy的技巧，在这里不做优劣比较，针对分类任务说明。</p>
<ul>
<li><strong>微调方式一</strong>，最常用，只替换掉最后一层fc layer，改成本任务里训练集的类别数目，然后不做其余特殊处理，直接开始训练；</li>
<li><strong>微调方式二</strong>，在微调一的基础上，freeze backbone的参数，只更新（预训练）新的fc layer的参数（更新的参数量少，训练更快）到收敛为止，之后再放开所有层的参数，再一起训练；</li>
<li><strong>微调方式三</strong>，在微调方式二预训练fc layer之后或者直接就是微调方式一，可选择接上差分学习率（discriminative learning  rates）即更新backbone参数和新fc  layer的参数所使用的学习率是不一致的，一般可选择差异10倍，理由是backbone的参数是基于imagenet训练的，参数足够优秀同时泛化性也会更好，所以是希望得到微调即可，不需要太大的变化。 </li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam([&#123;&apos;params&apos;: model.backbone.parameters(), &apos;lr&apos;: 3e-5&#125;, &#123;&apos;params&apos;: model.fc.parameters(), &apos;lr&apos;: 3e-4&#125;, ])</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>微调方式四</strong>，freeze浅层，训练深层（如可以不更新resnet前两个resnet block的参数，只更新其余的参数，一样是为了增强泛化，减少过拟合）。</li>
</ul>
<p>4.<strong>Find the best init_lr</strong>，前面说到3e-4在Adam是较优的init_lr，那么如何寻找最好的init_lr？</p>
<ul>
<li>出自fastai, lr_find()，其原理就是选取loss function仍在明显降低的较大的学习速率，优劣性其实也是相对而言，不一定都是最好的。</li>
</ul>
<p>5.<strong>learing rate warmup</strong>，理论解释可以参 <a href="https://www.zhihu.com/question/338066667" target="_blank" rel="noopener">https://www.zhihu.com/question/338066667</a></p>
<p>6.如果模型太大的同时你的GPU显存又不够大，那么设置的batch size就会太小，如何在有限的资源里提升多一点？</p>
<ul>
<li>梯度累计（gradient accumulation），其实就是积累多个batch的梯度之后，再进行梯度的回传做参数的更新，变相的增大了训练的batch size，但缺点是对Batch Normalization没影响的。。</li>
<li>如果你卡多，这时可以使用多卡并行训练，但要使用syncbn（跨卡同步bn），即增大了batch size，又对Batch Normalization起到相同的作用。</li>
</ul>
<h2 id="分类赛技巧"><a href="#分类赛技巧" class="headerlink" title="分类赛技巧"></a>分类赛技巧</h2><p>1.<strong>label smoothing</strong> </p>
<p>分类任务的标签是one-hot形式，交叉熵会不断地去拟合这个真实概率，在数据不够充足的情况下拟合one-hot容易形成过拟合，因为one-hot会鼓励正确类别与所属类别之间的差异性尽可能大，但其实有不少类别之间是极为相似的。label smoothing的做法其实就是将hard label变成soft label。</p>
<p>2.<strong>topk-loss(OHEM)</strong> </p>
<p>OHEM最初是在目标检测上提出来的，但其实思想是所有领域任务都通用的。意思就是提取当前batch里top k大的loss的均值作为当前batch的loss，进行梯度的计算和回传。其insight也很简单，就是一种hard  mining的方法，一个batch里会有easy sample和hard sample，easy  sample对网络的更新作用较小（loss值小，梯度也会小），而hard  sample的作用会更大（loss值大，梯度值也会大），所以topk-loss就是提取hard sample。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = criterion(logits, truth)</span><br><span class="line">loss,_ = loss.topk(k=..)     </span><br><span class="line">loss = loss.mean()</span><br></pre></td></tr></table></figure>
<p>3.<strong>weighted loss</strong> </p>
<p>weighted loss其实也算是一种hard  mining的方法，只不过这种是人为地认为哪种类别样本更加hard，哪种类别样本更加easy。也就是说人为对不同类别的loss进行进行一个权重的设置，比如0,1类更难，设置权重为1.2，2类更容易，设置权重为0.8。。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = [<span class="number">1.2</span>, <span class="number">1.2</span>, <span class="number">0.8</span>]</span><br><span class="line">class_weights = torch.FloatTensor(weights).to(device)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss(weight=class_weights)</span><br></pre></td></tr></table></figure>
<p>4.<strong>dual pooling</strong> </p>
<p>这种是在模型层进行改造的一种小trick了，常见的做法：global max/average pooling + fc layer，这里试concat(global max-pooling, global  average pooling) + fc layer，其实就是为了丰富特征层，max pooling更加关注重要的局部特征，而average  pooling试更加关注全局的特征。不一定有效，我试过不少次，有效的次数比较少，但不少人喜欢这样用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">res18</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes)</span>:</span></span><br><span class="line">        super(res18, self).__init__()</span><br><span class="line">        self.base = resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line">        self.feature = nn.Sequential(</span><br><span class="line">            self.base.conv1,</span><br><span class="line">            self.base.bn1,</span><br><span class="line">            self.base.relu,</span><br><span class="line">            self.base.maxpool,</span><br><span class="line">            self.base.layer1,</span><br><span class="line">            self.base.layer2,</span><br><span class="line">            self.base.layer3,</span><br><span class="line">            self.base.layer4</span><br><span class="line">        )</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line">        self.reduce_layer = nn.Conv2d(<span class="number">1024</span>, <span class="number">512</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc  = nn.Sequential(</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line">            )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        bs = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.feature(x)</span><br><span class="line">        x1 = self.avg_pool(x).view(bs, <span class="number">-1</span>)</span><br><span class="line">        x2 = self.max_pool(x).view(bs, <span class="number">-1</span>)</span><br><span class="line">        x1 = self.avg_pool(x)</span><br><span class="line">        x2 = self.max_pool(x)</span><br><span class="line">        x = torch.cat([x1, x2], dim=<span class="number">1</span>)</span><br><span class="line">        x = self.reduce_layer(x).view(bs, <span class="number">-1</span>)</span><br><span class="line">        logits = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></table></figure>
<p>5.<strong>margin-based softmax</strong></p>
<ul>
<li>在人脸识别领域，基于margin的softmax loss其实就是对softmax loss的一系列魔改（large margin  softmax、NormFace、AM-softmax、CosFace、ArcFace等等），增加类间  margin，当然也有其它的特点，如weight  norm和基于余弦角度的优化等等。其共同目标都是为了获得一个更加具有区分度的feature，不易过拟合。</li>
<li>一个比较多同学忽略的点是，如果使用了margin-based  softmax，往往连同开源repo里默认的超参数也一起使用了，比如s=32.0，m=0.5，但其实这两个参数的设定都是有一定的缘由，比如s值象征着超球体的体积，如果类别数较多，那么s应该设置大点。如果你没有很好的直觉，那grid search一波，搜索到适合的s和m值也不会花很多时间。</li>
</ul>
<p>6.<strong>Lovasz loss</strong></p>
<ul>
<li>这个loss本来是出于分割任务上的，其优化的是IOU，但你如果仔细观察lovasz传入的logit和truth，可以发现是和multi label classification类似，logit和truth都是由多个1值的one-hot形式。所以在多标签分类任务上，其实是可以用lovasz loss来进行优化的，出自（Bestfitting）(<a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/c/human-protein-atlas-image-classification/discussion/78109%23latest-676029" target="_blank" rel="noopener">https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/78109#latest-676029</a>)</li>
</ul>
<h2 id="分类赛技巧（openset-检索）"><a href="#分类赛技巧（openset-检索）" class="headerlink" title="分类赛技巧（openset/检索）"></a>分类赛技巧（openset/检索）</h2><p>1.<strong>BNNeck</strong>(出自罗浩博士的Bag of Tricks and A Strong Baseline for Deep Person Re-identification )，知乎链接<a href="https://zhuanlan.zhihu.com/p/61831669" target="_blank" rel="noopener">一个更加强力的ReID Baseline</a>， 其实就是在feature层和fc layer之间增加一层Batch Normalization  layer，然后在retrieval的时候，使用BN后的feature再做一个l2 norm，也就是retrieval with Cosine  distance。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">res50</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes)</span>:</span></span><br><span class="line">        super(res50, self).__init__()</span><br><span class="line">        resnet = resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line">        self.backbone = torch.nn.Sequential(</span><br><span class="line">                        resnet.conv1,</span><br><span class="line">                        resnet.bn1,</span><br><span class="line">                        resnet.relu,</span><br><span class="line">                        resnet.layer1,</span><br><span class="line">                        resnet.layer2,</span><br><span class="line">                        resnet.layer3,</span><br><span class="line">                        resnet.layer4</span><br><span class="line">        )</span><br><span class="line">        self.pool = torch.nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line">        self.bnneck = nn.BatchNorm1d(<span class="number">2048</span>)</span><br><span class="line">        self.bnneck.bias.requires_grad_(<span class="keyword">False</span>)  <span class="comment"># no shift</span></span><br><span class="line">        self.classifier = nn.Linear(<span class="number">2048</span>, num_classes, bias=<span class="keyword">False</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.backbone(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        feat = x.view(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        feat = self.bnneck(feat)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> nn.functional.normalize(feat, dim=<span class="number">1</span>, p=<span class="number">2</span>)</span><br><span class="line">        x = self.classifier(feat)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>2.<strong>margin-based softmax</strong>（上面已经说到）</p>
<p>3.<strong>triplet loss + softmax loss</strong>，结合metric learning，对feature进行多个loss的优化，triplet loss也是可以有很多的花样，Batch Hard Triplet Loss，是针对triplet loss的一种hard mining方法。</p>
<p>4.<strong>IBN</strong>，切换带有IBN block的backbone，搜图（open-set）往往test和train是不同场景下的数据，IBN  block当初提出是为了提高针对不同场景下的模型泛化性能，提升跨域（cross domain）能力，在reid下的实验，IBN表现优异。</p>
<p>5.<strong>center loss</strong> </p>
<p>6.<strong>Gem，generalized mean pooling</strong>，出自Fine-tuning CNN Image Retrieval with No Human Annotation，提出的是一种可学习的pooling layer，可提高检索性能，代码出自 <a href="https://link.zhihu.com/?target=https%3A//github.com/tuananh1007/CNN-Image-Retrieval-in-PyTorch/blob/master/cirtorch/layers/pooling.py" target="_blank" rel="noopener">https://github.com/tuananh1007/CNN-Image-Retrieval-in-PyTorch/blob/master/cirtorch/layers/pooling.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GeM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, p=<span class="number">3</span>, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(GeM,self).__init__()</span><br><span class="line">        self.p = Parameter(torch.ones(<span class="number">1</span>)*p)</span><br><span class="line">        self.eps = eps</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> LF.gem(x, p=self.p, eps=self.eps)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.__class__.__name__ + <span class="string">'('</span> + <span class="string">'p='</span> + <span class="string">'&#123;:.4f&#125;'</span>.format(self.p.data.tolist()[<span class="number">0</span>]) + <span class="string">', '</span> + <span class="string">'eps='</span> + str(self.eps) + <span class="string">')'</span></span><br></pre></td></tr></table></figure>
<p>7.<strong>global feature + local features</strong> 将全局特征和多个局部特征一起融合，其实就是一种暴力融合特征的方法，对提升精度有一定的帮助，就是耗时相对只使用global  feature来说很多点，此种方法可参考在reid常用的PCB(Beyond Part Models: Person Retrieval  with Refined Part Pooling)或MGN(Learning Discriminative Features with  Multiple Granularities for Person Re-Identification)方法</p>
<p>8.<strong>re-ranking</strong>，是一种在首次获取检索图的候选图里做一次重新排序，获得更加精准的检索，相对比较耗时间，不适合现实场景，适合比赛刷精度。</p>
<h2 id="分割赛技巧"><a href="#分割赛技巧" class="headerlink" title="分割赛技巧"></a>分割赛技巧</h2><p>1.<strong>Unet</strong> Unet可以说是在kaggle的语义分割赛里的一个较优的选择，许多top solution都是使用了Unet，FPN也是一个非常不错的选择。<br>2.<strong>Unet的魔改</strong><br>   现在有个开源库其实是已经集成了许多不同分割网络，表现也是相对不错的，如果觉得自己修改比较困难，或者自己改得不够好，可以尝试使用这个库<strong>segmentation_models_pytorch</strong></p>
<ul>
<li>很多top solution都是修改Unet的Decoder，最常见的就是增加scse block和Hypercolumn  block，也有一些是使用了CBAM（Convolutional Block Attention  Module，bestfitting比较喜欢用）或BAM（Bottleneck attention  module），这些注意力block一般是放在decoder不同stage出来的feature后面，因为注意力机制往往都是来优化feature的。</li>
<li>dual head(multi task learning)，也就是构造一个end2end带有分割与分类的模型。同时，多任务学习往往会降低模型过拟合的程度，并可以提升模型的性能。</li>
</ul>
<p><img src="/2019/10/25/kaggle-solution/1.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> segmentation_models_pytorch <span class="keyword">as</span> smp</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Res34_UNET</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes)</span>:</span></span><br><span class="line">        super(Res34_UNET, self).__init__()</span><br><span class="line">        self.model = smp.Unet(encoder_name=<span class="string">'resnet34'</span>, encoder_weights=<span class="string">'imagenet'</span>, classes=num_classes, activation=<span class="keyword">None</span>)</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        self.cls_head = nn.Linear(<span class="number">512</span>, num_classes, bias=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        global_features = self.model.encoder(x)</span><br><span class="line">        cls_feature = global_features[<span class="number">0</span>]</span><br><span class="line">        cls_feature = self.avgpool(cls_feature)</span><br><span class="line">        cls_feature = cls_feature.view(cls_feature.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        cls_feature = self.cls_head(cls_feature)</span><br><span class="line">        seg_feature = self.model.decoder(global_features)</span><br><span class="line">        <span class="keyword">return</span> seg_feature, cls_feature</span><br></pre></td></tr></table></figure>
<p>3.<strong>lovasz loss</strong> 之前在TGS Salt  Identification的适合，lovasz对分割的效果的表现真的是出类拔萃，相比bce或者dice等loss可以提高一个档次。但是最近的分割赛这个loss的表现就一般，猜测是优化不同metric，然后不同loss就会带来不同的效果，又或者是数据的问题。</p>
<p>4.<strong>dice loss for postive，bce loss for negtive</strong> 主要就是将分割任务划分两个任务：1. 分割任务，2. 分类任务 dice loss可以很好的优化模型的dice score，而bce  loss训练出来的分类器可以很好地找出negtive  sample，两者结合可以达到一种非常好效果，详细解说可以参考我之前的一个solution: <a href="https://zhuanlan.zhihu.com/p/93198713" target="_blank" rel="noopener">Kaggle Understanding Clouds 7th place总结</a></p>
<h2 id="通用技巧"><a href="#通用技巧" class="headerlink" title="通用技巧"></a>通用技巧</h2><p>1.<strong>TTA（Test Time Augmentation）</strong>  一种暴力测试的方法，将有效的增强方式得到多个不同的input，然后进行infer，得到多个结果进行融合，一般会比原始input会高不少。这种方法的缘由就是希望通过对input进行不同的变换方式获取多个不同的但重要的特征，然后可以得到多个具有差异性的预测结果。<br>2.<strong>多尺度训练，融合</strong> 在训练期间，随机输入多种尺度的图像进行训练，如（128<em>128，196</em>196，224<em>224，256</em>256，384*384等等）然后测试的时候可适当的选取其中某几个尺度表现优异的预测结果出来融合，这种方法其实就是为了提升模型对尺度的变换的鲁棒性，不易受尺度变换的影响。<br>3.<strong>Ensemble</strong></p>
<ul>
<li><strong>Snapshot Ensembles</strong>，这个方法常在与cycle learning rate的情况下使用，在不同cycle下，模型会产出多个不同的snapshot  weight（多个不同的局部最优，具有差异性），这时可以将这几个snapshot model一起进行推断，然后将预测结果进行平均融合。</li>
<li><strong>SWA, Stochastic Weight Averaging</strong>，随机权重平均，其实现原理当模型在训练时收敛到一定程度后，开始追踪每次epoch后得到的模型的平均值，有一个计算公式和当前的模型权重做一个平均得到一个最终的权重，提高泛化性能。</li>
<li><strong>stacking</strong>，在分类任务里，stacking是作为一种2nd level的ensemble方法，将多个“准而不同”的基分类器的预测集成与一身，再扔进去一个简单的分类器（mlp、logit  regression、simple  cnn，xgboost等）让其自己学习怎么将多个模型融合的收益做到最高。一般数据没有问题的话，stacking会更加稳定，不易过拟合，融合的收益也会更高。</li>
</ul>
<p>4.<strong>设计metric loss</strong>  许多小伙伴会有这样一个疑惑，比赛的评测metric往往和自己训练时所使用的loss优化方向不是那么一致。比如多标签分类里的metric是fbeta_score，但训练时是用了bce loss，经常可以看到val loss再收敛后会有一个反弹增大的过程，但此时val  fbeta_score是还在继续提升的。这时就可以针对metric来自行设计loss，比如fbeta loss就有。</p>
<p>5.<strong>semi-supervised learning</strong></p>
<ul>
<li><strong>recurssive pseudo-label（伪标签）</strong>，伪标签现在已经是kaggle赛里一个必备工具了，但是这是个非常危险的操作，如果没有筛选好的伪标签出来，容易造成模型过拟合伪标签里的许多噪声。比较安全的方法是：1. 筛选预测置信度高的样本作为伪标签，如分类里，再test里的预测概率是大于0.9的，则视为正确的预测，此时将其作为伪标签来使用。2.  帮第一次的伪标签扔进去训练集一起训练后，得到新的模型，按相同的规则再次挑一次伪标签出来。3. 如此不断循环多次，置信度的阈值可以适当作调整。</li>
<li><strong>mean teacher</strong>，在这里给涛哥在Recursion Cellular Image Classification第三名的方案做个广告，end2end semi-supervised learining pipeline。</li>
</ul>
<p><img src="/2019/10/25/kaggle-solution/2.jpg" alt=""></p>
<ul>
<li><strong>knowledge distillation（知识蒸馏）</strong>，此方法有助于提高小模型（student）的性能，将大模型（teacher）的预测作为soft label（用于学习teacher的模型信息）与truth（hard  label）扔进去给小模型一起学习，当然两个不同label的loss权重需要调一调。当然，蒸馏的方法有很多种，这只是其中一种最简单的方法。蒸馏不一定用于训练小模型，大模型之间也是可以一同使用的 </li>
</ul>
<h2 id="数据增强与预处理"><a href="#数据增强与预处理" class="headerlink" title="数据增强与预处理"></a>数据增强与预处理</h2><p>1.<strong>h/v flip(水平垂直翻转)</strong>，95%的情况下都是有效的，因为不怎么破坏图像空间信息。<br>2.<strong>random crop/center crop and resize</strong>，在原图进行crop之后再resize到指定的尺度。模型的感受野有限，有时会看不到图像中一些分布比较边缘或者是面积比较小的目标物体，crop过后其占比有更大，模型看到的机会也会更多。适用性也是比较大的。<br>3.<strong>random cutout/erasing(随机擦除)</strong>，其实就是为了随机擦除图像中局部特征，模型根据有限的特征也可以判断出其属性，可提高模型的泛化性。<br>4.<strong>AutoAugment</strong>，自己设定一些规则policy，让模型自己寻找合适的数据增强方式，需要消耗比较多的计算资源。<br>5.<strong>mixup</strong> 一种与数据无关的数据增强方式，即特征之间的线性插值应导致相关标签之间的线性插值，扩大训练分布。意思是两个不同的label的样本进行不同比例的线性插值融合，那么其label也应该是相同比例关系进行线性融合。（上图）<br>6.<strong>Class balance</strong> 主要就是针对数据不平衡的情况下进行的操作，一般是针对采样方法，或者在loss上做处理，如focal loss、weighted loss等。<br>7.<strong>图像预处理</strong>，许多看似有效的预处理操作，但是并不一定有效，如在医学领域的图像，许多肉眼观察良好的预处理的方式，实际上是破坏了原图真实类别关联的特征，这种方面需要相关领域知识。</p>
<h3 id="如何选择更好的backbone模型"><a href="#如何选择更好的backbone模型" class="headerlink" title="如何选择更好的backbone模型"></a>如何选择更好的backbone模型</h3><p>1.对于baseline，从<strong>resnet18/34 or efficientnet-B0</strong>起步，把所有work的技巧（loss/augmentation/metric/lr_schedule）调好之后，这时就应该大模型（deeper）；<br>2.当更好需要换模型的时候，是不是就需要自己去设计/构造新模型呢？其实在比赛的短期里，重新设计一个新的backbone出来是不提倡的，因为模型不仅要work，还要重新在imagenet上预训练，时间消耗巨大，不合适比赛；<br>3.由于学术界里，sota模型多之又多，那如何选择？从个人经验总结来看，比较推荐<strong>se-resnext50/101、SENet154</strong>（太大，很少用），带有se  block的resnet，都不同类型的任务都有一定的通用性，性价比也较高；efficientnet系列（最近在某些比赛里还优于se-resnext）可以上到B3,B5，有条件的B7都没问题。其他的sota模型，可以尝试Xception，inception-resnetV2等等。</p>
<h1 id="Competitions"><a href="#Competitions" class="headerlink" title="Competitions"></a>Competitions</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/27/pytorch_tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/27/pytorch_tutorial/" itemprop="url">Pytorch Tutorial</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-27T16:22:39+08:00">
                2019-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="0-安装"><a href="#0-安装" class="headerlink" title="0.安装"></a>0.安装</h1><h2 id="0-1-安装Anaconda3"><a href="#0-1-安装Anaconda3" class="headerlink" title="0.1 安装Anaconda3:"></a>0.1 安装Anaconda3:</h2><p><a href="https://blog.csdn.net/u012318074/article/details/77074665" target="_blank" rel="noopener">https://blog.csdn.net/u012318074/article/details/77074665</a><br>现在最新的版本所带的python版本应该是3.7 （找个3.6版本的也可以）</p>
<h2 id="0-2-安装Pytorch"><a href="#0-2-安装Pytorch" class="headerlink" title="0.2 安装Pytorch"></a>0.2 安装Pytorch</h2><p>在命令行输入<code>which pip</code>， 应该会显示pip在anaconda3下面，这时如果用pip命令装pytorch就是装在Anaconda3里面了。</p>
<p>在命令行输入<code>pip install torch==1.0.0</code>和 <code>pip install torchvision</code>进行安装，其中torch后面跟的是版本，现在最新的是1.2，有些代码要求的版本可能要低一些或高一些，就需要用<code>pip install torch==x.x.x</code>来降低或升高版本。</p>
<p>其他安装方式参见官网 <a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a></p>
<h2 id="0-3-安装Pycharm"><a href="#0-3-安装Pycharm" class="headerlink" title="0.3 安装Pycharm"></a>0.3 安装Pycharm</h2><p>在官网下载linux professional 版本 <a href="https://www.jetbrains.com/pycharm/download/#section=linux" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/download/#section=linux</a></p>
<p>学生是可以免费使用的，需要进行学生认证，时限是一年，第二年可以继续认证。<br>认证流程大概是<a href="https://blog.csdn.net/qq_36667170/article/details/79905198" target="_blank" rel="noopener">https://blog.csdn.net/qq_36667170/article/details/79905198</a> （我也没看过</p>
<p>下载完了之后在命令行输入<code>tar zxvf xxxx.tar.gz</code>解压压缩包</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/28/googlecloud/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/28/googlecloud/" itemprop="url">windows下载Google Cloud Platform上的数据</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-28T13:50:15+08:00">
                2019-06-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>以google的HDR+数据集为例：<br><a href="https://www.hdrplusdata.org/dataset.html" target="_blank" rel="noopener">https://www.hdrplusdata.org/dataset.html</a></p>
<h3 id="1-科学上网"><a href="#1-科学上网" class="headerlink" title="1. 科学上网"></a>1. 科学上网</h3><p><a href="https://www.4spaces.org/digitalocean-shadowsocks/" target="_blank" rel="noopener">https://www.4spaces.org/digitalocean-shadowsocks/</a><br><a href="https://github.com/shadowsocks/shadowsocks-gui" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-gui</a></p>
<h3 id="2-下载Google-Cloud-SDK"><a href="#2-下载Google-Cloud-SDK" class="headerlink" title="2.下载Google Cloud SDK"></a>2.下载Google Cloud SDK</h3><p><a href="https://cloud.google.com/sdk/docs/quickstart-windows" target="_blank" rel="noopener">https://cloud.google.com/sdk/docs/quickstart-windows</a></p>
<p>2.1 下载安装程序，安装过程中需要关掉ss<br>2.2 装完了会弹出一个命令行，开始init gcloud, 在输入y之前，打开ss。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">To continue, you must log in. Would you like to log in (Y/n)? Y</span><br></pre></td></tr></table></figure></p>
<p>然后就会弹出网站，根据提示登录您的 Google 用户帐号，然后点击允许以授权访问 Google Cloud Platform 资源。<br>然后选择一个 Cloud Platform 项目<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Pick cloud project to use:</span><br><span class="line"> [1] [my-project-1]</span><br><span class="line"> [2] [my-project-2]</span><br><span class="line"> ...</span><br><span class="line"> Please enter your numeric choice:</span><br></pre></td></tr></table></figure></p>
<p>成功完成设置步骤后显示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gcloud has now been configured!</span><br><span class="line">You can use [gcloud config] to change more gcloud settings.</span><br><span class="line"></span><br><span class="line">Your active configuration is: [default]</span><br></pre></td></tr></table></figure></p>
<h3 id="3-配置代理"><a href="#3-配置代理" class="headerlink" title="3. 配置代理"></a>3. 配置代理</h3><p><a href="https://cloud.google.com/sdk/docs/proxy-settings" target="_blank" rel="noopener">https://cloud.google.com/sdk/docs/proxy-settings</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gcloud config set proxy/type socks5</span><br><span class="line">gcloud config set proxy/address 127.0.0.1</span><br><span class="line">gcloud config set proxy/port 1080</span><br></pre></td></tr></table></figure>
<h3 id="4-下载"><a href="#4-下载" class="headerlink" title="4 下载"></a>4 下载</h3><p>cmd中输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gsutil -m cp -r gs://hdrplusdata/20171106_subset .</span><br></pre></td></tr></table></figure></p>
<p>会把数据下在当前目录下</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/05/something_for_pycharm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/05/something_for_pycharm/" itemprop="url">Something for Pycharm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-05T16:18:07+08:00">
                2019-06-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h1><table>
<thead>
<tr>
<th style="text-align:center"></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Alt+Shift+F9</td>
<td>Debug</td>
</tr>
<tr>
<td style="text-align:center">Alt+Shitf+F10</td>
<td>Run</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+W</td>
<td>选中一小块</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+Y</td>
<td>删除整行</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+Shift+F</td>
<td>全局搜索</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+F</td>
<td>搜索</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+/</td>
<td>注释</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+R</td>
<td>替换</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+c</td>
<td>复制整行</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+Alt+I</td>
<td>自动缩进</td>
</tr>
<tr>
<td style="text-align:center">Tab</td>
<td>缩进</td>
</tr>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h1 id="遇到的一些问题"><a href="#遇到的一些问题" class="headerlink" title="遇到的一些问题"></a>遇到的一些问题</h1><p><strong>打开后不是原来的路径，如下图所示</strong><br><img src="/2019/06/05/something_for_pycharm/2.png" alt=""></p>
<p>在setting -&gt; Project: XXX -&gt; project structure 里面 有个Add Content Root 把root删了重新添加自己想要的root<br><img src="/2019/06/05/something_for_pycharm/3.png" alt=""></p>
<hr>
<p><strong>Project Interpreter 的选择</strong><br>如果是用anaconda里的pip装的一些东西，选择existing environment 选择anaconda安装目录下bin/python3<br><img src="/2019/06/05/something_for_pycharm/1.png" alt=""></p>
<hr>
<p><strong>pycharm同一目录下无法import明明已经存在的.py文件</strong><br>mark  Directory as <em>source root</em><br><a href="https://blog.csdn.net/l8947943/article/details/79874180" target="_blank" rel="noopener">https://blog.csdn.net/l8947943/article/details/79874180</a></p>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/09/Deconvolution-and-Checkerboard-Artifacts/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/09/Deconvolution-and-Checkerboard-Artifacts/" itemprop="url">Deconvolution and Checkerboard Artifacts</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-09T15:29:08+08:00">
                2019-04-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Deconvolution can easily have “uneven overlap,”（不均匀重叠）putting more of the metaphorical paint in some places than others. In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top). While the network could, in principle, carefully learn weights to avoid this  — as we’ll discuss in more detail later — in practice neural networks struggle to avoid it completely.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/29/study-tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/29/study-tensorflow/" itemprop="url">Study Tensorflow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-29T16:10:26+08:00">
                2019-03-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://www.tensorfly.cn/" target="_blank" rel="noopener">http://www.tensorfly.cn/</a></p>
<h1 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h1><ul>
<li>使用图 (graph) 来表示计算任务.</li>
<li>在被称之为 <code>会话 (Session)</code> 的上下文 (context) 中执行图.</li>
<li>使用 tensor 表示数据.</li>
<li>通过 <code>变量 (Variable)</code> 维护状态.</li>
<li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.</li>
</ul>
<h3 id="构建图"><a href="#构建图" class="headerlink" title="构建图"></a>构建图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点</span></span><br><span class="line"><span class="comment"># 加到默认图中.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 构造器的返回值代表该常量 op 的返回值.</span></span><br><span class="line">matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建另外一个常量 op, 产生一个 2x1 矩阵.</span></span><br><span class="line">matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.</span></span><br><span class="line"><span class="comment"># 返回值 'product' 代表矩阵乘法的结果.</span></span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br></pre></td></tr></table></figure>
<h3 id="启动图"><a href="#启动图" class="headerlink" title="启动图"></a>启动图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动默认图.</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 sess 的 'run()' 方法来执行矩阵乘法 op, 传入 'product' 作为该方法的参数. </span></span><br><span class="line"><span class="comment"># 上面提到, 'product' 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回</span></span><br><span class="line"><span class="comment"># 矩阵乘法 op 的输出.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 返回值 'result' 是一个 numpy `ndarray` 对象.</span></span><br><span class="line">result = sess.run(product)</span><br><span class="line"><span class="keyword">print</span> result</span><br><span class="line"><span class="comment"># ==&gt; [[ 12.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 任务完成, 关闭会话.</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p><code>ession</code> 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 “with” 代码块 来自动完成关闭动作.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  result = sess.run([product])</span><br><span class="line">  <span class="keyword">print</span> result</span><br></pre></td></tr></table></figure>
<p>如果机器上有超过一个可用的 GPU, 除第一个外的其它 GPU 默认是不参与计算的. 为了让 TensorFlow 使用这些 GPU, 你必须将 op 明确指派给它们执行. <code>with...Device</code> 语句用来指派特定的 CPU 或 GPU 执行操作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">"/gpu:1"</span>):</span><br><span class="line">    matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line">    matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line">    product = tf.matmul(matrix1, matrix2)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>设备用字符串进行标识. 目前支持的设备包括:</p>
<ul>
<li><code>&quot;/cpu:0&quot;</code>: 机器的 CPU.</li>
<li><code>&quot;/gpu:0&quot;</code>: 机器的第一个 GPU, 如果有的话.</li>
<li><code>&quot;/gpu:1&quot;</code>: 机器的第二个 GPU, 以此类推.</li>
</ul>
<h3 id="交互式使用"><a href="#交互式使用" class="headerlink" title="交互式使用"></a>交互式使用</h3><p>文档中的 Python 示例使用一个会话 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#Session" target="_blank" rel="noopener"><code>Session</code></a> 来 启动图, 并调用 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#Session.run" target="_blank" rel="noopener"><code>Session.run()</code></a> 方法执行操作.</p>
<p>为了便于使用诸如 <a href="http://ipython.org/" target="_blank" rel="noopener">IPython</a> 之类的 Python 交互环境, 可以使用 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#InteractiveSession" target="_blank" rel="noopener"><code>InteractiveSession</code></a> 代替 <code>Session</code> 类, 使用 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Tensor.eval" target="_blank" rel="noopener"><code>Tensor.eval()</code></a> 和 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Operation.run" target="_blank" rel="noopener"><code>Operation.run()</code></a> 方法代替 <code>Session.run()</code>. 这样可以避免使用一个变量来持有会话.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入一个交互式 TensorFlow 会话.</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">a = tf.constant([<span class="number">3.0</span>, <span class="number">3.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用初始化器 initializer op 的 run() 方法初始化 'x' </span></span><br><span class="line">x.initializer.run()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一个减法 sub op, 从 'x' 减去 'a'. 运行减法 op, 输出结果 </span></span><br><span class="line">sub = tf.sub(x, a)</span><br><span class="line"><span class="keyword">print</span> sub.eval()</span><br><span class="line"><span class="comment"># ==&gt; [-2. -1.]</span></span><br></pre></td></tr></table></figure>
<h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个变量, 初始化为标量 0.</span></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">"counter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 op, 其作用是使 state 增加 1</span></span><br><span class="line"></span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图后, 变量必须先经过`初始化` (init) op 初始化,</span></span><br><span class="line"><span class="comment"># 首先必须增加一个`初始化` op 到图中.</span></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图, 运行 op</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># 运行 'init' op</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  <span class="comment"># 打印 'state' 的初始值</span></span><br><span class="line">  <span class="keyword">print</span> sess.run(state)</span><br><span class="line">  <span class="comment"># 运行 op, 更新 'state', 并打印 'state'</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    sess.run(update)</span><br><span class="line">    <span class="keyword">print</span> sess.run(state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure>
<h3 id="Fetch"><a href="#Fetch" class="headerlink" title="Fetch"></a>Fetch</h3><p>为了取回操作的输出内容, 可以在使用 <code>Session</code> 对象的 <code>run()</code> 调用 执行图时, 传入一些 tensor, 这些 tensor 会帮助你取回结果. 在之前的例子里, 我们只取回了单个节点 <code>state</code>, 但是你也可以取回多个 tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">input2 = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">input3 = tf.constant(<span class="number">5.0</span>)</span><br><span class="line">intermed = tf.add(input2, input3)</span><br><span class="line">mul = tf.mul(input1, intermed)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session():</span><br><span class="line">  result = sess.run([mul, intermed])</span><br><span class="line">  <span class="keyword">print</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]</span></span><br></pre></td></tr></table></figure>
<h3 id="Feed"><a href="#Feed" class="headerlink" title="Feed"></a>Feed</h3><p>上述示例在计算图中引入了 tensor, 以常量或变量的形式存储. TensorFlow 还提供了 feed 机制, 该机制 可以临时替代图中的任意操作中的 tensor 可以对图中任何操作提交补丁, 直接插入一个 tensor.</p>
<p>feed 使用一个 tensor 值临时替换一个操作的输出结果. 你可以提供 feed 数据作为 <code>run()</code> 调用的参数. feed 只在调用它的方法内有效, 方法结束, feed 就会消失. 最常见的用例是将某些特殊的操作指定为 “feed” 操作, 标记的方法是使用 tf.placeholder() 为这些操作创建占位符.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.types.float32)</span><br><span class="line">input2 = tf.placeholder(tf.types.float32)</span><br><span class="line">output = tf.mul(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="keyword">print</span> sess.run([output], feed_dict=&#123;input1:[<span class="number">7.</span>], input2:[<span class="number">2.</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># [array([ 14.], dtype=float32)]</span></span><br></pre></td></tr></table></figure>
<h1 id="基础教程"><a href="#基础教程" class="headerlink" title="基础教程"></a>基础教程</h1><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure>
<h3 id="构建Softmax-回归模型"><a href="#构建Softmax-回归模型" class="headerlink" title="构建Softmax 回归模型"></a>构建Softmax 回归模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>这里的<code>x</code>和<code>y</code>并不是特定的值，相反，他们都只是一个<code>占位符</code>，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。</p>
<p><code>变量</code>需要通过seesion初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个<code>变量</code>,可以一次性为所有<code>变量</code>完成此操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.initialize_all_variables())</span><br></pre></td></tr></table></figure>
<h4 id="类别预测与损失函数："><a href="#类别预测与损失函数：" class="headerlink" title="类别预测与损失函数："></a>类别预测与损失函数：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = tf.nn.softmax(tf.matmul(x,W) + b)</span><br><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span><br></pre></td></tr></table></figure>
<h4 id="训练模型："><a href="#训练模型：" class="headerlink" title="训练模型："></a>训练模型：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。</p>
<p>返回的<code>train_step</code>操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行<code>train_step</code>来完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p>每一步迭代，我们都会加载50个训练样本，然后执行一次<code>train_step</code>，并通过<code>feed_dict</code>将<code>x</code> 和 <code>y_</code>张量<code>占位符</code>用训练训练数据替代。</p>
<p>注意，在计算图中，你可以用<code>feed_dict</code>来替代任何张量，并不仅限于替换<code>占位符</code>。</p>
<h4 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h4><p><code>tf.argmax</code> 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：<code>[True, False, True, True]</code>变为<code>[1,0,1,1]</code>，计算出平均值为<code>0.75</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br></pre></td></tr></table></figure>
<p>最后，我们可以计算出在测试数据上的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="构建一个多层卷积网络"><a href="#构建一个多层卷积网络" class="headerlink" title="构建一个多层卷积网络"></a>构建一个多层卷积网络</h3><h4 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h4><p>这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<h4 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a>卷积和池化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])  <span class="comment"># BxWxHxC</span></span><br><span class="line"></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>我们用一个<code>placeholder</code>来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的<code>tf.nn.dropout</code>操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br></pre></td></tr></table></figure>
<h4 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy)</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Cifar"><a href="#Cifar" class="headerlink" title="Cifar"></a>Cifar</h2><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="API"><a href="#API" class="headerlink" title="API"></a>API</h1><h2 id="Build-Model"><a href="#Build-Model" class="headerlink" title="Build Model"></a>Build Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(name,</span><br><span class="line">    shape=<span class="keyword">None</span>,</span><br><span class="line">    dtype=<span class="keyword">None</span>,</span><br><span class="line">    initializer=<span class="keyword">None</span>,</span><br><span class="line">    regularizer=<span class="keyword">None</span>,</span><br><span class="line">    trainable=<span class="keyword">True</span>,</span><br><span class="line">    collections=<span class="keyword">None</span>,</span><br><span class="line">    caching_device=<span class="keyword">None</span>,</span><br><span class="line">    partitioner=<span class="keyword">None</span>,</span><br><span class="line">    validate_shape=<span class="keyword">True</span>,</span><br><span class="line">    use_resource=<span class="keyword">None</span>,</span><br><span class="line">    custom_getter=<span class="keyword">None</span>,</span><br><span class="line">    constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>创建或返回给定名称的变量</p>
<p><code>tf.variable_scope()</code></p>
<p><a href="https://www.cnblogs.com/MY0213/p/9208503.html" target="_blank" rel="noopener">https://www.cnblogs.com/MY0213/p/9208503.html</a></p>
<p>用来指定变量的作用域，作为变量名的前缀，支持嵌套</p>
<p><code>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</code></p>
<p><code>tf.nn.bias_add</code></p>
<p><code>tf.contrib.layers.batch_norm</code></p>
<p><code>tf.conv2d_transpose(value, filter, output_shape, strides, padding=&quot;SAME&quot;, data_format=&quot;NHWC&quot;, name=None)</code></p>
<p><code>tf.nn.tanh()</code></p>
<p><code>tf.reduce_mean(inputs, [1, 2], name=&#39;global_average_pooling&#39;, keepdims=True)</code></p>
<p><code>tf.image.resize_bilinear(image_level_features, inputs_size, name=&#39;upsample&#39;)</code></p>
<h2 id="Operate"><a href="#Operate" class="headerlink" title="Operate"></a>Operate</h2><p><code>tf.slice(inputs,begin,size,name=&#39;&#39;)</code></p>
<p>inputs：可以是list,array,tensor<br>begin：n维列表，begin[i] 表示从inputs中第i维抽取数据时，相对0的起始偏移量，也就是从第i维的begin[i]开始抽取数据<br>size：n维列表，size[i]表示要抽取的第i维元素的数目</p>
<p><code>tf.concat([tensor1, tensor2, tensor3,...], axis)</code></p>
<h2 id="tf-logging"><a href="#tf-logging" class="headerlink" title="tf.logging"></a>tf.logging</h2><p><code>tf.logging.set_verbosity (tf.logging.INFO)</code></p>
<p>设计日志级别</p>
<p><code>tf.logging.info(msg, *args, **kwargs)</code></p>
<p>记录INFO级别的日志. </p>
<h2 id="tf-gfile"><a href="#tf-gfile" class="headerlink" title="tf.gfile"></a>tf.gfile</h2><p><a href="https://blog.csdn.net/pursuit_zhangyu/article/details/80557958" target="_blank" rel="noopener">https://blog.csdn.net/pursuit_zhangyu/article/details/80557958</a></p>
<h2 id="tf-contrib-slim"><a href="#tf-contrib-slim" class="headerlink" title="tf.contrib.slim"></a>tf.contrib.slim</h2><p><a href="https://www.2cto.com/kf/201706/649266.html" target="_blank" rel="noopener">https://www.2cto.com/kf/201706/649266.html</a></p>
<h2 id="tt-contrib-layers"><a href="#tt-contrib-layers" class="headerlink" title="tt.contrib.layers"></a>tt.contrib.layers</h2><p><a href="https://www.cnblogs.com/zyly/p/8995119.html" target="_blank" rel="noopener">https://www.cnblogs.com/zyly/p/8995119.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/28/segmentation-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/28/segmentation-paper/" itemprop="url">segmentation-paper</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-28T09:34:08+08:00">
                2019-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Sematic-Segmentation"><a href="#Sematic-Segmentation" class="headerlink" title="Sematic Segmentation"></a>Sematic Segmentation</h1><p><strong>[FCN - CVPR2015]</strong></p>
<p>-不含全连接层(fc)的<strong>全卷积(fully conv)网络</strong>。可适应任意尺寸输入。 </p>
<p>-增大数据尺寸的<strong>反卷积(deconv)层</strong>。能够输出精细的结果。 </p>
<p>-结合不同深度层结果的<strong>跳级(skip)结构</strong>。同时确保鲁棒性和精确性。</p>
<p><img src="/2019/03/28/segmentation-paper/1.png" alt=""></p>
<hr>
<p><strong>[U-Net - MICCAI2015]</strong></p>
<p><img src="/2019/03/28/segmentation-paper/2.png" alt=""></p>
<hr>
<p><strong>[RefineNet - CVPR2017]</strong></p>
<p>-The deconvolution operations are not able to recover the low-level visual features which are lost after the down-sampling operation in the convolution forward stage</p>
<p>-Dilated convolutions introduce a coarse sub-sampling of features, which potentially leads to a loss of important details</p>
<p>-RefineNet provides a generic means to fuse coarse high-level semantic features with finer-grained low-level features to generate high-resolution semantic feature maps.</p>
<p><img src="/2019/03/28/segmentation-paper/3.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/4.png" alt=""></p>
<hr>
<p><strong>[PSPNet - CVPR2017]</strong></p>
<p>-Current FCN based model is lack of suitable strategy to utilize global scene category clues.</p>
<p>-Global context information along with sub-region context is helpful in this regard to distinguish among various categories.</p>
<p><img src="/2019/03/28/segmentation-paper/5.png" alt=""></p>
<hr>
<p><strong>[GCN - CVPR2017] Large Kernel Matters——Improve Semantic Segmentation by Global Convolutional Network</strong></p>
<p><img src="/2019/03/28/segmentation-paper/10.png" alt=""></p>
<hr>
<p><strong>[DFN - CVPR2018] Learning a Discriminative Feature Network for Semantic Segmentation </strong></p>
<p><img src="/2019/03/28/segmentation-paper/11.png" alt=""></p>
<hr>
<p><strong>[BiSeNet - ECCV2018] BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</strong></p>
<p><img src="/2019/03/28/segmentation-paper/12.png" alt=""></p>
<p>-Spatial Path (SP) and Context Path (CP). As their names imply, the two components are devised to confront with the loss of spatial information and shrinkage of receptive field respectively.</p>
<p>-SP: three layers, each layer includes a convolution with stride = 2, followed by batch normalization and ReLU.</p>
<p>-CP: utilizes lightweight model and global average pooling to provide large receptive field</p>
<p>-loss function:</p>
<hr>
<p><strong>[ICNet - ECCV2018] ICNet for Real-Time Semantic Segmentation on High-Resolution Images</strong></p>
<hr>
<p><strong>[DFANet - CVPR2019] DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation</strong></p>
<p><img src="/2019/03/28/segmentation-paper/14.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/13.png" alt=""></p>
<hr>
<p><strong>[DeepLabv1 - ICLR2015]</strong></p>
<p>-Atrous Convolution</p>
<p><strong>[DeepLabv2]</strong></p>
<p>-Atrous Spatial Pyramid Pooling (ASPP)</p>
<p><img src="/2019/03/28/segmentation-paper/6.png" alt=""></p>
<p><strong>[DeepLabv3]</strong></p>
<p><img src="/2019/03/28/segmentation-paper/9.png" alt=""></p>
<p><strong>[DeepLabv3+ - ECCV2018]</strong></p>
<p><img src="/2019/03/28/segmentation-paper/8.png" alt=""></p>
<hr>
<h2 id="“Attention”-in-Segmentation"><a href="#“Attention”-in-Segmentation" class="headerlink" title="“Attention” in Segmentation"></a>“Attention” in Segmentation</h2><p><strong>[NLNet - CVPR2018]  Non-local Neural Networks</strong></p>
<p>-Capturing long-range dependencies is of central importance in deep neural networks. Intuitively, a non-local operation computes the response at a position as a weighted sum of the features at all positions in the input feature maps.</p>
<p>-Generic non-local operation:<br><img src="/2019/03/28/segmentation-paper/15.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/17.png" alt=""><br><img src="/2019/03/28/segmentation-paper/18.png" alt=""><br><img src="/2019/03/28/segmentation-paper/16.png" alt=""><br>-HWxHW与HWx512做矩阵乘，前一个可以理解为每一行是一个点的f， 然后与512维中每个点相乘，对于每个通道上，用的f值是一样的，可以理解为spatial attention。</p>
<p><strong>[DANet - CVPR2019] Dual Attention Network for Scene Segmentation</strong></p>
<p>-Introduces a self-attention mechanism to capture features dependencies in the spatial and channel dimensions</p>
<p><img src="/2019/03/28/segmentation-paper/23.png" alt=""><br><img src="/2019/03/28/segmentation-paper/24.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/25.png" alt=""></p>
<p><strong>[CCNet - ICCV2019] CCNet: Criss-Cross Attention for Semantic Segmentation</strong></p>
<p>-The current no-local operation, can be alternatively replaced by two consecutive criss-cross operations, in which each one only has sparse connections (H + W - 1) for each position in the feature maps. By serially stacking two criss-cross attention modules, it can collect contextual information from all pixels. The decomposition greatly reduce the complexity in time and space from O((HxW)x(HxW)) to O((HxW)x(H +W - 1)).</p>
<p><img src="/2019/03/28/segmentation-paper/19.png" alt=""><br><img src="/2019/03/28/segmentation-paper/20.png" alt=""></p>
<p>-The details of criss-cross attention module:<br><img src="/2019/03/28/segmentation-paper/21.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/22.png" alt=""></p>
<p>-</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/28/super-resolution-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/28/super-resolution-paper/" itemprop="url">super-resolution-paper</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-28T09:33:55+08:00">
                2019-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="PSNR-Oriented-Approach"><a href="#PSNR-Oriented-Approach" class="headerlink" title="PSNR Oriented Approach"></a>PSNR Oriented Approach</h1><p><strong>[SRCNN - ECCV2014] Learning a Deep Convolutional Network for Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/1.png" alt=""></p>
<p>-The training data set is synthesized by extracting nonoverlapping dense patches of size 32x32 from the HR images. The LR input patches are first downsampled and then upsampled using bicubic interpolation having the same size as the high-resolution output image.</p>
<p>-MSE loss</p>
<hr>
<p><strong>[VDSR - CVPR2016] Accurate Image Super-Resolution Using Very Deep Convolutional Networks</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/2.png" alt=""></p>
<p>-VGG</p>
<p>-To speed-up the training: (1) learn a residual mapping that generates the difference between the HR and LR image instead of directly generating a HR image. (2) gradients are clipped with in the range [-θ, θ]. These allow very high learning rates.</p>
<hr>
<p><strong>[ESPCN - CVPR2016] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/4.png" alt=""></p>
<p>-Propose an efficient sub-pixel convolution layer to learn the upscaling operation for image and video super-resolution.</p>
<p>-Sub-pixel convolution : <a href="https://blog.csdn.net/bbbeoy/article/details/81085652" target="_blank" rel="noopener">https://blog.csdn.net/bbbeoy/article/details/81085652</a><br>First, use a convolution outputs H x W x Crr. Second, use periodic shuffling to rearange it to Hr x Wr x C </p>
<hr>
<p><strong>[DRCN - CVPR2016] Deeply-Recursive Convolutional Network for Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/13.png" alt=""></p>
<p>-Supervise all recursions in order to alleviate the effect of vanishing/exploding gradients.</p>
<p>-Add a layer skip from input to the reconstruction net.</p>
<hr>
<p><strong>[FSRCNN - ECCV2016] Accelerating the Super-Resolution Convolutional Neural Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/3.png" alt=""></p>
<p>-Use deconvolution as a post-upsamling step instead of upsampling the original LR image as a pre-processing step.</p>
<p>-Use PReLU instead of ReLU.</p>
<p>-Use the 91-image dataset [1] with another 100 images collected from the internet. Data augmentation such as rotation, flipping, and scaling is also employed to increase the number<br>of images by 19 times.<br><em>[1] J. Yang, J.Wright, T. S. Huang, and Y. Ma, “Image super-resolution via sparse representation,” TIP, 2010.</em></p>
<hr>
<p><strong>[RED-Net - NIPS2016] Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections</strong></p>
<hr>
<p><strong>[LapSRN - CVPR2017] Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/19.png" alt=""></p>
<p>-Progressively predicts residual images at log2(S) levels where S is the scale factor.</p>
<p>-Loss function:<br><img src="/2019/03/28/super-resolution-paper/20.png" alt=""></p>
<p>-3x3conv, 4x4deconv, lrelu.</p>
<hr>
<p><strong>[DRRN - CVPR2017] Image Super-Resolution via Deep Recursive Residual Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/14.png" alt=""></p>
<p>-Global and local residual learning</p>
<p>-Recursive block consisting of several residual units, and the weight set is shared among these residual units.</p>
<p>-For one block,  a multi-path structure is used and all the residual units share the same input for the identity branch.</p>
<hr>
<p><strong>[EDSR - CVPR2017W] Enhanced Deep Residual Networks for Single Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/5.png" alt=""></p>
<p>-Modify SRResNet:</p>
<p>(1) Remove Batch Normalization layers (from each residual block) and ReLU activation (outside residual blocks). Since batch normalization layers normalize the features, they get rid of range flexibility from networks by normalizing the features. Furthermore, GPU memory usage is also sufficiently reduced since the batch normalization layers consume the same amount of memory as the preceding convolutional layers. Consequently, we can build up a larger model.</p>
<p><img src="/2019/03/28/super-resolution-paper/6.png" alt=""></p>
<p>(2) Increasing F(the number of feature channels) instead of B(thenumber of layers) can maximize the model capacity when considering limited computational resources. Use residual scaling to stabilize the training procedure. In EDSR, set B = 32, F = 256, scaling factor=0.1.</p>
<p>-When training our model for upsampling factor x3 and x4, we initialize the model parameters with pre-trained x2 network.</p>
<p>-MDSR (Multi scale model) (B = 80 and F = 64)</p>
<p><img src="/2019/03/28/super-resolution-paper/7.png" alt=""></p>
<p>-L1 loss provides better convergence than L2.</p>
<hr>
<p><strong>[BTSRN - CVPR2017W] Balanced Two-Stage Residual Networks for Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/11.png" alt=""></p>
<p>-Only 10 residual blocks to ensure the efficiency. (6 for lr stage, 4 for hr stage)</p>
<p><img src="/2019/03/28/super-resolution-paper/12.png" alt=""></p>
<p>-For the up-sampling layers, the element sum of nearest neighbor up-sampling and deconvolution is employed. To reduce the artifacts, the stride and size of kernels are equal to scaling factor for x2 and x3, and two x2 up-sampling are applied for x4 scaling.</p>
<p>-Batch normalization is not suitable for super-resolution task. Because super-resolution is a regressing task, the target outputs are highly correlated to inputs first order statistics, while batch normalization makes the networks invariant to data re-centering and re-scaling.</p>
<p>-Predict the residual images.  L2 loss.</p>
<hr>
<p><strong>[SelNet - CVPR2017W] A Deep Convolutional Neural Network with Selection Units for Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/31.png" alt=""></p>
<hr>
<p><strong>[MemNet - ICCV2017] MemNet: A Persistent Memory Network for Image Restoration</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/17.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/16.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/15.png" alt=""></p>
<p>-Gate unit: 1x1conv</p>
<hr>
<p><strong>[SRDenseNet - ICCV2017] Image Super-Resolution Using Dense Skip Connections</strong><br><img src="/2019/03/28/super-resolution-paper/18.png" alt=""></p>
<hr>
<p><strong>[RDN - CVPR2018] Residual Dense Network for Image Super-Resolution</strong></p>
<p><em>加一点MemNet，加一点SRDenseNet</em></p>
<p><img src="/2019/03/28/super-resolution-paper/21.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/22.png" alt=""></p>
<p>-l1 loss which has been demonstrated to be more powerful for performance and convergence.</p>
<hr>
<p><strong>[DBPN - CVPR2018] Deep Back-Projection Networks For Super-Resolution</strong></p>
<p>-DBPN:<br><img src="/2019/03/28/super-resolution-paper/23.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/24.png" alt=""></p>
<p>-D-DBPN:<br><img src="/2019/03/28/super-resolution-paper/25.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/26.png" alt=""></p>
<p>-Avoid dropout and batch norm, use 1 x 1 convolution layer as feature pooling and dimensional<br>reduction instead.</p>
<p>-The projection unit uses large sized filters such as 8 x 8 and 12 x 12. In other existing networks, the use of largesized filter is avoided because it slows down the convergence speed and might produce sub-optimal results. However, iterative utilization of our projection units enables the network to suppress this limitation and to perform better performance on large scaling factor even with shallow net works.</p>
<hr>
<p><strong>[IDN - CVPR2018] Fast and Accurate Single Image Super-Resolution via Information Distillation Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/27.png" alt=""></p>
<p>-Enhancement unit (each of convs is followed by LReLU)<br><img src="/2019/03/28/super-resolution-paper/28.png" alt=""></p>
<p>-Compression unit: 1x1conv</p>
<p>-First train the network with MAE loss and then fine-tune it by MSE loss</p>
<hr>
<p><strong>[CARN - ECCV2018] Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network</strong></p>
<p><em>其实就是把DRRN里resnet的连线改为densenet的连线，加了些1x1conv</em></p>
<p>-Global cascade:</p>
<p><img src="/2019/03/28/super-resolution-paper/8.png" alt=""></p>
<p>-Cascading block, local cascade and efficient residual (residual-E) block.  And to further reduce the parameters, make the parameters of the Cascading blocks shared, effectively making the blocks recursive.<br><img src="/2019/03/28/super-resolution-paper/9.png" alt=""></p>
<p>-Cascading on both the local and global levels has two advantages: 1) The model<br>incorporates features from multiple layers, which allows learning multi-level rep-<br>resentations. 2) Multi-level cascading connection behaves as multi-level shortcut<br>connections that quickly propagate information from lower to higher layers (and<br>vice-versa, in case of back-propagation).</p>
<p><img src="/2019/03/28/super-resolution-paper/10.png" alt=""></p>
<hr>
<p><strong>[RCAN - ECCV2018] Image Super-Resolution Using Very Deep Residual Channel Attention Networks</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/29.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/30.png" alt=""></p>
<hr>
<p><strong>[SRRAM - arXiv1811] RAM: Residual Attention Module for Single Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/32.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/33.png" alt=""></p>
<p>-Channel Attention: since SR ultimately aims at restoring high-frequency components of images,<br>it is more reasonable for attention maps to be determined using high-frequency statistics about the channels. To this end, we choose to use the variance rather than the average for the pooling method</p>
<p>-Spatial Attention: use depth-wise convolution</p>
<h1 id="GAN-based-Approach"><a href="#GAN-based-Approach" class="headerlink" title="GAN based Approach"></a>GAN based Approach</h1><p><strong>[SRGAN - CVPR2017] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/34.png" alt=""></p>
<p>-Perceptual loss function<br><img src="/2019/03/28/super-resolution-paper/35.png" alt=""></p>
<p>-content loss<br>With φ<sub>ij</sub> we indicate the feature map obtained by the j-th convolution (after activation) before the i-th maxpooling layer within the VGG19 network.<br><img src="/2019/03/28/super-resolution-paper/36.png" alt=""></p>
<p>-adverarial loss<br><img src="/2019/03/28/super-resolution-paper/37.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/38.png" alt=""></p>
<hr>
<p><strong>[EhanceNet - ICCV2017] EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis</strong></p>
<hr>
<p><strong>[SRFeat - ECCV2018] SRFeat: Single Image Super-Resolution with Feature Discrimination</strong></p>
<p>-While GAN-based SISR methods show dramatic improvements over previous approaches in terms of perceptual quality, they often tend to produce less meaningful high-frequency noise in super-resolved images. We argue that this is because the most dominant difference between super-resolved images and real HR images is high-frequency information, where super-resolved images obtained by minimizing pixel-wise errors lack high-frequency details. The simplest way for a discriminator to distinguish super-resolved images from real HR images could be simply inspecting the presence of high-frequency components in a given im- age, and the simplest way for a generator to fool the discriminator would be to put arbitrary high-frequency noise into result images.</p>
<p><img src="/2019/03/28/super-resolution-paper/39.png" alt=""></p>
<p>-First pre-train using MSE loss, then go adversarial training.</p>
<p><img src="/2019/03/28/super-resolution-paper/40.png" alt=""></p>
<p>-L<sub>p</sub>: perceptual Similarity Loss.<br>L<sup>i</sup><sub>a</sub> : image gan loss.<br><img src="/2019/03/28/super-resolution-paper/42.png" alt=""><br>L<sup>f</sup><sub>a</sub> : feature gan loss.<br><img src="/2019/03/28/super-resolution-paper/41.png" alt=""></p>
<hr>
<p><strong>[ESRGAN - ECCV2018W] Enhanced Super-Resolution Generative Adversarial Networks</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/43.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/44.png" alt=""></p>
<p>-Net Architecture:<br>Like EDSR: When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. We empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework.</p>
<p>Residual leaning.<br>Smaller initialization</p>
<p>-Relativistic Discriminator<br>A relativistic discriminator tries to predict the probability that a real image x<sub>r</sub> is relatively more realistic than a fake one x<sub>f</sub><br><img src="/2019/03/28/super-resolution-paper/45.png" alt=""><br>where E[ ] represents the operation of taking average for all fake or real data in the mini-batch.</p>
<p>The discriminator loss is then defined as:<br><img src="/2019/03/28/super-resolution-paper/46.png" alt=""><br>The adversarial loss for generator is in a symmetrical form:<br><img src="/2019/03/28/super-resolution-paper/47.png" alt=""></p>
<p>It is observed that the adversarial loss for generator contains both xr and xf . Therefore, our generator benefits from the gradients from both generated data and real data in adversarial training, while in SRGAN only generated part takes effect. The experiment shows this modification of discriminator helps to learn sharper edges and more detailed textures.</p>
<p>-Perceptual Loss:<br>Develop a more effective perceptual loss by constraining on features before activation rather than after activation. Two reasons: first, the activated features are very sparse; second, using features after activation also causes inconsistent reconstructed brightness</p>
<p>-total loss for the generator<br><img src="/2019/03/28/super-resolution-paper/48.png" alt=""></p>
<p>-Network Interpolation<br>To remove unpleasant noise in GAN-based methods while maintain a good perceptual quality<br><img src="/2019/03/28/super-resolution-paper/49.png" alt=""></p>
<p><strong>[RankSRGAN - ICCV2019] RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution</strong></p>
<h1 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h1><p>Four steps: feature extraction, alignment, fusion, and reconstruction.</p>
<p><strong>[VESPCN - CVPR2017] Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation</strong></p>
<p><strong>[SPMC - ICCV2017] Detail-revealing Deep Video Super-resolution </strong></p>
<p><strong>[FRVSR - CVPR2018] Frame-Recurrent Video Super-Resolution</strong></p>
<p><strong>[DUF - CVPR2018] Deep Video Super-Resolution Network Using Dynamic Upsampling FiltersWithout Explicit Motion Compensation</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/50.png" alt=""></p>
<p>-<em>Dynamic Upsampling Filters</em><br><img src="/2019/03/28/super-resolution-paper/51.png" alt=""></p>
<p>First, a set of input LR frames {Xt−N:t+N} (7 frames in our network: N = 3) is fed into the dynamic filter generation network. The trained network outputs a set of r<sup>2</sup>HW upsampling filters Ft of a certain size (5 × 5 in our network).(F的大小为: 5 x 5 x r<sup>2</sup>HW, 这里的r是scale factor)</p>
<p>然后对于原图中的一点，用周围5x5的点与F中5x5xr<sup>2</sup>依次相乘，得到rxr个点，从而upsampling了r倍。</p>
<p>-<em>Residual Learning</em><br>The result after applying the dynamic upsampling filters alone lacks sharpness as it is still a weighted sum of input pixels.To address this, we additionally estimate a residual image to increase high frequency details</p>
<p>-<em>Network Design</em><br>3D convolutional layers / filter and residual generation network are designed to share most of the weights</p>
<p><strong>[EDVR - CVPRW2019] EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</strong></p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Pan Sicheng</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Pan Sicheng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
