<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Good Good Study">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Good Good Study">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Good Good Study">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Good Good Study</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Good Good Study</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/25/kaggle-solution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/25/kaggle-solution/" itemprop="url">kaggle_solution</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-25T15:12:27+08:00">
                2019-10-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/27/pytorch_tutorial/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/27/pytorch_tutorial/" itemprop="url">Pytorch Tutorial</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-27T16:22:39+08:00">
                2019-09-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="0-安装"><a href="#0-安装" class="headerlink" title="0.安装"></a>0.安装</h1><h2 id="0-1-安装Anaconda3"><a href="#0-1-安装Anaconda3" class="headerlink" title="0.1 安装Anaconda3:"></a>0.1 安装Anaconda3:</h2><p><a href="https://blog.csdn.net/u012318074/article/details/77074665" target="_blank" rel="noopener">https://blog.csdn.net/u012318074/article/details/77074665</a><br>现在最新的版本所带的python版本应该是3.7 （找个3.6版本的也可以）</p>
<h2 id="0-2-安装Pytorch"><a href="#0-2-安装Pytorch" class="headerlink" title="0.2 安装Pytorch"></a>0.2 安装Pytorch</h2><p>在命令行输入<code>which pip</code>， 应该会显示pip在anaconda3下面，这时如果用pip命令装pytorch就是装在Anaconda3里面了。</p>
<p>在命令行输入<code>pip install torch==1.0.0</code>和 <code>pip install torchvision</code>进行安装，其中torch后面跟的是版本，现在最新的是1.2，有些代码要求的版本可能要低一些或高一些，就需要用<code>pip install torch==x.x.x</code>来降低或升高版本。</p>
<p>其他安装方式参见官网 <a href="https://pytorch.org/" target="_blank" rel="noopener">https://pytorch.org/</a></p>
<h2 id="0-3-安装Pycharm"><a href="#0-3-安装Pycharm" class="headerlink" title="0.3 安装Pycharm"></a>0.3 安装Pycharm</h2><p>在官网下载linux professional 版本 <a href="https://www.jetbrains.com/pycharm/download/#section=linux" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/download/#section=linux</a></p>
<p>学生是可以免费使用的，需要进行学生认证，时限是一年，第二年可以继续认证。<br>认证流程大概是<a href="https://blog.csdn.net/qq_36667170/article/details/79905198" target="_blank" rel="noopener">https://blog.csdn.net/qq_36667170/article/details/79905198</a> （我也没看过</p>
<p>下载完了之后在命令行输入<code>tar zxvf xxxx.tar.gz</code>解压压缩包</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/28/googlecloud/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/28/googlecloud/" itemprop="url">windows下载Google Cloud Platform上的数据</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-28T13:50:15+08:00">
                2019-06-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>以google的HDR+数据集为例：<br><a href="https://www.hdrplusdata.org/dataset.html" target="_blank" rel="noopener">https://www.hdrplusdata.org/dataset.html</a></p>
<h3 id="1-科学上网"><a href="#1-科学上网" class="headerlink" title="1. 科学上网"></a>1. 科学上网</h3><p><a href="https://www.4spaces.org/digitalocean-shadowsocks/" target="_blank" rel="noopener">https://www.4spaces.org/digitalocean-shadowsocks/</a><br><a href="https://github.com/shadowsocks/shadowsocks-gui" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-gui</a></p>
<h3 id="2-下载Google-Cloud-SDK"><a href="#2-下载Google-Cloud-SDK" class="headerlink" title="2.下载Google Cloud SDK"></a>2.下载Google Cloud SDK</h3><p><a href="https://cloud.google.com/sdk/docs/quickstart-windows" target="_blank" rel="noopener">https://cloud.google.com/sdk/docs/quickstart-windows</a></p>
<p>2.1 下载安装程序，安装过程中需要关掉ss<br>2.2 装完了会弹出一个命令行，开始init gcloud, 在输入y之前，打开ss。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">To continue, you must log in. Would you like to log in (Y/n)? Y</span><br></pre></td></tr></table></figure></p>
<p>然后就会弹出网站，根据提示登录您的 Google 用户帐号，然后点击允许以授权访问 Google Cloud Platform 资源。<br>然后选择一个 Cloud Platform 项目<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Pick cloud project to use:</span><br><span class="line"> [1] [my-project-1]</span><br><span class="line"> [2] [my-project-2]</span><br><span class="line"> ...</span><br><span class="line"> Please enter your numeric choice:</span><br></pre></td></tr></table></figure></p>
<p>成功完成设置步骤后显示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gcloud has now been configured!</span><br><span class="line">You can use [gcloud config] to change more gcloud settings.</span><br><span class="line"></span><br><span class="line">Your active configuration is: [default]</span><br></pre></td></tr></table></figure></p>
<h3 id="3-配置代理"><a href="#3-配置代理" class="headerlink" title="3. 配置代理"></a>3. 配置代理</h3><p><a href="https://cloud.google.com/sdk/docs/proxy-settings" target="_blank" rel="noopener">https://cloud.google.com/sdk/docs/proxy-settings</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gcloud config set proxy/type socks5</span><br><span class="line">gcloud config set proxy/address 127.0.0.1</span><br><span class="line">gcloud config set proxy/port 1080</span><br></pre></td></tr></table></figure>
<h3 id="4-下载"><a href="#4-下载" class="headerlink" title="4 下载"></a>4 下载</h3><p>cmd中输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gsutil -m cp -r gs://hdrplusdata/20171106_subset .</span><br></pre></td></tr></table></figure></p>
<p>会把数据下在当前目录下</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/05/something_for_pycharm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/05/something_for_pycharm/" itemprop="url">Something for Pycharm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-05T16:18:07+08:00">
                2019-06-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h1><table>
<thead>
<tr>
<th style="text-align:center"></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Alt+Shift+F9</td>
<td>Debug</td>
</tr>
<tr>
<td style="text-align:center">Alt+Shitf+F10</td>
<td>Run</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+W</td>
<td>选中一小块</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+Y</td>
<td>删除整行</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+Shift+F</td>
<td>全局搜索</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+F</td>
<td>搜索</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+/</td>
<td>注释</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+R</td>
<td>替换</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+c</td>
<td>复制整行</td>
</tr>
<tr>
<td style="text-align:center">Ctrl+Alt+I</td>
<td>自动缩进</td>
</tr>
<tr>
<td style="text-align:center">Tab</td>
<td>缩进</td>
</tr>
<tr>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h1 id="遇到的一些问题"><a href="#遇到的一些问题" class="headerlink" title="遇到的一些问题"></a>遇到的一些问题</h1><p><strong>打开后不是原来的路径，如下图所示</strong><br><img src="/2019/06/05/something_for_pycharm/2.png" alt=""></p>
<p>在setting -&gt; Project: XXX -&gt; project structure 里面 有个Add Content Root 把root删了重新添加自己想要的root<br><img src="/2019/06/05/something_for_pycharm/3.png" alt=""></p>
<hr>
<p><strong>Project Interpreter 的选择</strong><br>如果是用anaconda里的pip装的一些东西，选择existing environment 选择anaconda安装目录下bin/python3<br><img src="/2019/06/05/something_for_pycharm/1.png" alt=""></p>
<hr>
<p><strong>pycharm同一目录下无法import明明已经存在的.py文件</strong><br>mark  Directory as <em>source root</em><br><a href="https://blog.csdn.net/l8947943/article/details/79874180" target="_blank" rel="noopener">https://blog.csdn.net/l8947943/article/details/79874180</a></p>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/09/Deconvolution-and-Checkerboard-Artifacts/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/09/Deconvolution-and-Checkerboard-Artifacts/" itemprop="url">Deconvolution and Checkerboard Artifacts</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-09T15:29:08+08:00">
                2019-04-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Deconvolution can easily have “uneven overlap,”（不均匀重叠）putting more of the metaphorical paint in some places than others. In particular, deconvolution has uneven overlap when the kernel size (the output window size) is not divisible by the stride (the spacing between points on the top). While the network could, in principle, carefully learn weights to avoid this  — as we’ll discuss in more detail later — in practice neural networks struggle to avoid it completely.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/29/study-tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/29/study-tensorflow/" itemprop="url">Study Tensorflow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-29T16:10:26+08:00">
                2019-03-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://www.tensorfly.cn/" target="_blank" rel="noopener">http://www.tensorfly.cn/</a></p>
<h1 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h1><ul>
<li>使用图 (graph) 来表示计算任务.</li>
<li>在被称之为 <code>会话 (Session)</code> 的上下文 (context) 中执行图.</li>
<li>使用 tensor 表示数据.</li>
<li>通过 <code>变量 (Variable)</code> 维护状态.</li>
<li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.</li>
</ul>
<h3 id="构建图"><a href="#构建图" class="headerlink" title="构建图"></a>构建图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点</span></span><br><span class="line"><span class="comment"># 加到默认图中.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 构造器的返回值代表该常量 op 的返回值.</span></span><br><span class="line">matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建另外一个常量 op, 产生一个 2x1 矩阵.</span></span><br><span class="line">matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个矩阵乘法 matmul op , 把 'matrix1' 和 'matrix2' 作为输入.</span></span><br><span class="line"><span class="comment"># 返回值 'product' 代表矩阵乘法的结果.</span></span><br><span class="line">product = tf.matmul(matrix1, matrix2)</span><br></pre></td></tr></table></figure>
<h3 id="启动图"><a href="#启动图" class="headerlink" title="启动图"></a>启动图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动默认图.</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用 sess 的 'run()' 方法来执行矩阵乘法 op, 传入 'product' 作为该方法的参数. </span></span><br><span class="line"><span class="comment"># 上面提到, 'product' 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回</span></span><br><span class="line"><span class="comment"># 矩阵乘法 op 的输出.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 返回值 'result' 是一个 numpy `ndarray` 对象.</span></span><br><span class="line">result = sess.run(product)</span><br><span class="line"><span class="keyword">print</span> result</span><br><span class="line"><span class="comment"># ==&gt; [[ 12.]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 任务完成, 关闭会话.</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p><code>ession</code> 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 “with” 代码块 来自动完成关闭动作.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  result = sess.run([product])</span><br><span class="line">  <span class="keyword">print</span> result</span><br></pre></td></tr></table></figure>
<p>如果机器上有超过一个可用的 GPU, 除第一个外的其它 GPU 默认是不参与计算的. 为了让 TensorFlow 使用这些 GPU, 你必须将 op 明确指派给它们执行. <code>with...Device</code> 语句用来指派特定的 CPU 或 GPU 执行操作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">"/gpu:1"</span>):</span><br><span class="line">    matrix1 = tf.constant([[<span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line">    matrix2 = tf.constant([[<span class="number">2.</span>],[<span class="number">2.</span>]])</span><br><span class="line">    product = tf.matmul(matrix1, matrix2)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>设备用字符串进行标识. 目前支持的设备包括:</p>
<ul>
<li><code>&quot;/cpu:0&quot;</code>: 机器的 CPU.</li>
<li><code>&quot;/gpu:0&quot;</code>: 机器的第一个 GPU, 如果有的话.</li>
<li><code>&quot;/gpu:1&quot;</code>: 机器的第二个 GPU, 以此类推.</li>
</ul>
<h3 id="交互式使用"><a href="#交互式使用" class="headerlink" title="交互式使用"></a>交互式使用</h3><p>文档中的 Python 示例使用一个会话 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#Session" target="_blank" rel="noopener"><code>Session</code></a> 来 启动图, 并调用 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#Session.run" target="_blank" rel="noopener"><code>Session.run()</code></a> 方法执行操作.</p>
<p>为了便于使用诸如 <a href="http://ipython.org/" target="_blank" rel="noopener">IPython</a> 之类的 Python 交互环境, 可以使用 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/client.html#InteractiveSession" target="_blank" rel="noopener"><code>InteractiveSession</code></a> 代替 <code>Session</code> 类, 使用 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Tensor.eval" target="_blank" rel="noopener"><code>Tensor.eval()</code></a> 和 <a href="http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Operation.run" target="_blank" rel="noopener"><code>Operation.run()</code></a> 方法代替 <code>Session.run()</code>. 这样可以避免使用一个变量来持有会话.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入一个交互式 TensorFlow 会话.</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">a = tf.constant([<span class="number">3.0</span>, <span class="number">3.0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用初始化器 initializer op 的 run() 方法初始化 'x' </span></span><br><span class="line">x.initializer.run()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加一个减法 sub op, 从 'x' 减去 'a'. 运行减法 op, 输出结果 </span></span><br><span class="line">sub = tf.sub(x, a)</span><br><span class="line"><span class="keyword">print</span> sub.eval()</span><br><span class="line"><span class="comment"># ==&gt; [-2. -1.]</span></span><br></pre></td></tr></table></figure>
<h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个变量, 初始化为标量 0.</span></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">"counter"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 op, 其作用是使 state 增加 1</span></span><br><span class="line"></span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line">update = tf.assign(state, new_value)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图后, 变量必须先经过`初始化` (init) op 初始化,</span></span><br><span class="line"><span class="comment"># 首先必须增加一个`初始化` op 到图中.</span></span><br><span class="line">init_op = tf.initialize_all_variables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动图, 运行 op</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># 运行 'init' op</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line">  <span class="comment"># 打印 'state' 的初始值</span></span><br><span class="line">  <span class="keyword">print</span> sess.run(state)</span><br><span class="line">  <span class="comment"># 运行 op, 更新 'state', 并打印 'state'</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    sess.run(update)</span><br><span class="line">    <span class="keyword">print</span> sess.run(state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br></pre></td></tr></table></figure>
<h3 id="Fetch"><a href="#Fetch" class="headerlink" title="Fetch"></a>Fetch</h3><p>为了取回操作的输出内容, 可以在使用 <code>Session</code> 对象的 <code>run()</code> 调用 执行图时, 传入一些 tensor, 这些 tensor 会帮助你取回结果. 在之前的例子里, 我们只取回了单个节点 <code>state</code>, 但是你也可以取回多个 tensor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">input2 = tf.constant(<span class="number">2.0</span>)</span><br><span class="line">input3 = tf.constant(<span class="number">5.0</span>)</span><br><span class="line">intermed = tf.add(input2, input3)</span><br><span class="line">mul = tf.mul(input1, intermed)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session():</span><br><span class="line">  result = sess.run([mul, intermed])</span><br><span class="line">  <span class="keyword">print</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]</span></span><br></pre></td></tr></table></figure>
<h3 id="Feed"><a href="#Feed" class="headerlink" title="Feed"></a>Feed</h3><p>上述示例在计算图中引入了 tensor, 以常量或变量的形式存储. TensorFlow 还提供了 feed 机制, 该机制 可以临时替代图中的任意操作中的 tensor 可以对图中任何操作提交补丁, 直接插入一个 tensor.</p>
<p>feed 使用一个 tensor 值临时替换一个操作的输出结果. 你可以提供 feed 数据作为 <code>run()</code> 调用的参数. feed 只在调用它的方法内有效, 方法结束, feed 就会消失. 最常见的用例是将某些特殊的操作指定为 “feed” 操作, 标记的方法是使用 tf.placeholder() 为这些操作创建占位符.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">input1 = tf.placeholder(tf.types.float32)</span><br><span class="line">input2 = tf.placeholder(tf.types.float32)</span><br><span class="line">output = tf.mul(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="keyword">print</span> sess.run([output], feed_dict=&#123;input1:[<span class="number">7.</span>], input2:[<span class="number">2.</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># [array([ 14.], dtype=float32)]</span></span><br></pre></td></tr></table></figure>
<h1 id="基础教程"><a href="#基础教程" class="headerlink" title="基础教程"></a>基础教程</h1><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure>
<h3 id="构建Softmax-回归模型"><a href="#构建Softmax-回归模型" class="headerlink" title="构建Softmax 回归模型"></a>构建Softmax 回归模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="keyword">None</span>, <span class="number">784</span>])</span><br><span class="line">y_ = tf.placeholder(<span class="string">"float"</span>, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>,<span class="number">10</span>]))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br></pre></td></tr></table></figure>
<p>这里的<code>x</code>和<code>y</code>并不是特定的值，相反，他们都只是一个<code>占位符</code>，可以在TensorFlow运行某一计算时根据该占位符输入具体的值。</p>
<p><code>变量</code>需要通过seesion初始化后，才能在session中使用。这一初始化步骤为，为初始值指定具体值（本例当中是全为零），并将其分配给每个<code>变量</code>,可以一次性为所有<code>变量</code>完成此操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.initialize_all_variables())</span><br></pre></td></tr></table></figure>
<h4 id="类别预测与损失函数："><a href="#类别预测与损失函数：" class="headerlink" title="类别预测与损失函数："></a>类别预测与损失函数：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = tf.nn.softmax(tf.matmul(x,W) + b)</span><br><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y))</span><br></pre></td></tr></table></figure>
<h4 id="训练模型："><a href="#训练模型：" class="headerlink" title="训练模型："></a>训练模型：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>
<p>这一行代码实际上是用来往计算图上添加一个新操作，其中包括计算梯度，计算每个参数的步长变化，并且计算出新的参数值。</p>
<p>返回的<code>train_step</code>操作对象，在运行时会使用梯度下降来更新参数。因此，整个模型的训练可以通过反复地运行<code>train_step</code>来完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>]&#125;)</span><br></pre></td></tr></table></figure>
<p>每一步迭代，我们都会加载50个训练样本，然后执行一次<code>train_step</code>，并通过<code>feed_dict</code>将<code>x</code> 和 <code>y_</code>张量<code>占位符</code>用训练训练数据替代。</p>
<p>注意，在计算图中，你可以用<code>feed_dict</code>来替代任何张量，并不仅限于替换<code>占位符</code>。</p>
<h4 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h4><p><code>tf.argmax</code> 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>这里返回一个布尔数组。为了计算我们分类的准确率，我们将布尔值转换为浮点数来代表对、错，然后取平均值。例如：<code>[True, False, True, True]</code>变为<code>[1,0,1,1]</code>，计算出平均值为<code>0.75</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br></pre></td></tr></table></figure>
<p>最后，我们可以计算出在测试数据上的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> accuracy.eval(feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="构建一个多层卷积网络"><a href="#构建一个多层卷积网络" class="headerlink" title="构建一个多层卷积网络"></a>构建一个多层卷积网络</h3><h4 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h4><p>这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">  initial = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">  <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure>
<h4 id="卷积和池化"><a href="#卷积和池化" class="headerlink" title="卷积和池化"></a>卷积和池化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">                        strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">W_conv1 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>])  <span class="comment"># BxWxHxC</span></span><br><span class="line"></span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br></pre></td></tr></table></figure>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line"></span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br></pre></td></tr></table></figure>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>我们用一个<code>placeholder</code>来代表一个神经元的输出在dropout中保持不变的概率。这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 TensorFlow的<code>tf.nn.dropout</code>操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = tf.placeholder(<span class="string">"float"</span>)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br></pre></td></tr></table></figure>
<h4 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br></pre></td></tr></table></figure>
<h4 id="训练和评估模型"><a href="#训练和评估模型" class="headerlink" title="训练和评估模型"></a>训练和评估模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))</span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv,<span class="number">1</span>), tf.argmax(y_,<span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">  batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">  <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">    train_accuracy = accuracy.eval(feed_dict=&#123;</span><br><span class="line">        x:batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"step %d, training accuracy %g"</span>%(i, train_accuracy)</span><br><span class="line">  train_step.run(feed_dict=&#123;x: batch[<span class="number">0</span>], y_: batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> <span class="string">"test accuracy %g"</span>%accuracy.eval(feed_dict=&#123;</span><br><span class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Cifar"><a href="#Cifar" class="headerlink" title="Cifar"></a>Cifar</h2><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h1 id="API"><a href="#API" class="headerlink" title="API"></a>API</h1><h2 id="Build-Model"><a href="#Build-Model" class="headerlink" title="Build Model"></a>Build Model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(name,</span><br><span class="line">    shape=<span class="keyword">None</span>,</span><br><span class="line">    dtype=<span class="keyword">None</span>,</span><br><span class="line">    initializer=<span class="keyword">None</span>,</span><br><span class="line">    regularizer=<span class="keyword">None</span>,</span><br><span class="line">    trainable=<span class="keyword">True</span>,</span><br><span class="line">    collections=<span class="keyword">None</span>,</span><br><span class="line">    caching_device=<span class="keyword">None</span>,</span><br><span class="line">    partitioner=<span class="keyword">None</span>,</span><br><span class="line">    validate_shape=<span class="keyword">True</span>,</span><br><span class="line">    use_resource=<span class="keyword">None</span>,</span><br><span class="line">    custom_getter=<span class="keyword">None</span>,</span><br><span class="line">    constraint=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>创建或返回给定名称的变量</p>
<p><code>tf.variable_scope()</code></p>
<p><a href="https://www.cnblogs.com/MY0213/p/9208503.html" target="_blank" rel="noopener">https://www.cnblogs.com/MY0213/p/9208503.html</a></p>
<p>用来指定变量的作用域，作为变量名的前缀，支持嵌套</p>
<p><code>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</code></p>
<p><code>tf.nn.bias_add</code></p>
<p><code>tf.contrib.layers.batch_norm</code></p>
<p><code>tf.conv2d_transpose(value, filter, output_shape, strides, padding=&quot;SAME&quot;, data_format=&quot;NHWC&quot;, name=None)</code></p>
<p><code>tf.nn.tanh()</code></p>
<p><code>tf.reduce_mean(inputs, [1, 2], name=&#39;global_average_pooling&#39;, keepdims=True)</code></p>
<p><code>tf.image.resize_bilinear(image_level_features, inputs_size, name=&#39;upsample&#39;)</code></p>
<h2 id="Operate"><a href="#Operate" class="headerlink" title="Operate"></a>Operate</h2><p><code>tf.slice(inputs,begin,size,name=&#39;&#39;)</code></p>
<p>inputs：可以是list,array,tensor<br>begin：n维列表，begin[i] 表示从inputs中第i维抽取数据时，相对0的起始偏移量，也就是从第i维的begin[i]开始抽取数据<br>size：n维列表，size[i]表示要抽取的第i维元素的数目</p>
<p><code>tf.concat([tensor1, tensor2, tensor3,...], axis)</code></p>
<h2 id="tf-logging"><a href="#tf-logging" class="headerlink" title="tf.logging"></a>tf.logging</h2><p><code>tf.logging.set_verbosity (tf.logging.INFO)</code></p>
<p>设计日志级别</p>
<p><code>tf.logging.info(msg, *args, **kwargs)</code></p>
<p>记录INFO级别的日志. </p>
<h2 id="tf-gfile"><a href="#tf-gfile" class="headerlink" title="tf.gfile"></a>tf.gfile</h2><p><a href="https://blog.csdn.net/pursuit_zhangyu/article/details/80557958" target="_blank" rel="noopener">https://blog.csdn.net/pursuit_zhangyu/article/details/80557958</a></p>
<h2 id="tf-contrib-slim"><a href="#tf-contrib-slim" class="headerlink" title="tf.contrib.slim"></a>tf.contrib.slim</h2><p><a href="https://www.2cto.com/kf/201706/649266.html" target="_blank" rel="noopener">https://www.2cto.com/kf/201706/649266.html</a></p>
<h2 id="tt-contrib-layers"><a href="#tt-contrib-layers" class="headerlink" title="tt.contrib.layers"></a>tt.contrib.layers</h2><p><a href="https://www.cnblogs.com/zyly/p/8995119.html" target="_blank" rel="noopener">https://www.cnblogs.com/zyly/p/8995119.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/28/segmentation-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/28/segmentation-paper/" itemprop="url">segmentation-paper</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-28T09:34:08+08:00">
                2019-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Sematic-Segmentation"><a href="#Sematic-Segmentation" class="headerlink" title="Sematic Segmentation"></a>Sematic Segmentation</h1><p><strong>[FCN - CVPR2015]</strong></p>
<p>-不含全连接层(fc)的<strong>全卷积(fully conv)网络</strong>。可适应任意尺寸输入。 </p>
<p>-增大数据尺寸的<strong>反卷积(deconv)层</strong>。能够输出精细的结果。 </p>
<p>-结合不同深度层结果的<strong>跳级(skip)结构</strong>。同时确保鲁棒性和精确性。</p>
<p><img src="/2019/03/28/segmentation-paper/1.png" alt=""></p>
<hr>
<p><strong>[U-Net - MICCAI2015]</strong></p>
<p><img src="/2019/03/28/segmentation-paper/2.png" alt=""></p>
<hr>
<p><strong>[RefineNet - CVPR2017]</strong></p>
<p>-The deconvolution operations are not able to recover the low-level visual features which are lost after the down-sampling operation in the convolution forward stage</p>
<p>-Dilated convolutions introduce a coarse sub-sampling of features, which potentially leads to a loss of important details</p>
<p>-RefineNet provides a generic means to fuse coarse high-level semantic features with finer-grained low-level features to generate high-resolution semantic feature maps.</p>
<p><img src="/2019/03/28/segmentation-paper/3.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/4.png" alt=""></p>
<hr>
<p><strong>[PSPNet - CVPR2017]</strong></p>
<p>-Current FCN based model is lack of suitable strategy to utilize global scene category clues.</p>
<p>-Global context information along with sub-region context is helpful in this regard to distinguish among various categories.</p>
<p><img src="/2019/03/28/segmentation-paper/5.png" alt=""></p>
<hr>
<p><strong>[GCN - CVPR2017] Large Kernel Matters——Improve Semantic Segmentation by Global Convolutional Network</strong></p>
<p><img src="/2019/03/28/segmentation-paper/10.png" alt=""></p>
<hr>
<p><strong>[DFN - CVPR2018] Learning a Discriminative Feature Network for Semantic Segmentation </strong></p>
<p><img src="/2019/03/28/segmentation-paper/11.png" alt=""></p>
<hr>
<p><strong>[BiSeNet - ECCV2018] BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</strong></p>
<p><img src="/2019/03/28/segmentation-paper/12.png" alt=""></p>
<p>-Spatial Path (SP) and Context Path (CP). As their names imply, the two components are devised to confront with the loss of spatial information and shrinkage of receptive field respectively.</p>
<p>-SP: three layers, each layer includes a convolution with stride = 2, followed by batch normalization and ReLU.</p>
<p>-CP: utilizes lightweight model and global average pooling to provide large receptive field</p>
<p>-loss function:</p>
<hr>
<p><strong>[ICNet - ECCV2018] ICNet for Real-Time Semantic Segmentation on High-Resolution Images</strong></p>
<hr>
<p><strong>[DFANet - CVPR2019] DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation</strong></p>
<p><img src="/2019/03/28/segmentation-paper/14.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/13.png" alt=""></p>
<hr>
<p><strong>[DeepLabv1 - ICLR2015]</strong></p>
<p>-Atrous Convolution</p>
<p><strong>[DeepLabv2]</strong></p>
<p>-Atrous Spatial Pyramid Pooling (ASPP)</p>
<p><img src="/2019/03/28/segmentation-paper/6.png" alt=""></p>
<p><strong>[DeepLabv3]</strong></p>
<p><img src="/2019/03/28/segmentation-paper/9.png" alt=""></p>
<p><strong>[DeepLabv3+ - ECCV2018]</strong></p>
<p><img src="/2019/03/28/segmentation-paper/8.png" alt=""></p>
<hr>
<h2 id="“Attention”-in-Segmentation"><a href="#“Attention”-in-Segmentation" class="headerlink" title="“Attention” in Segmentation"></a>“Attention” in Segmentation</h2><p><strong>[NLNet - CVPR2018]  Non-local Neural Networks</strong></p>
<p>-Capturing long-range dependencies is of central importance in deep neural networks. Intuitively, a non-local operation computes the response at a position as a weighted sum of the features at all positions in the input feature maps.</p>
<p>-Generic non-local operation:<br><img src="/2019/03/28/segmentation-paper/15.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/17.png" alt=""><br><img src="/2019/03/28/segmentation-paper/18.png" alt=""><br><img src="/2019/03/28/segmentation-paper/16.png" alt=""><br>-HWxHW与HWx512做矩阵乘，前一个可以理解为每一行是一个点的f， 然后与512维中每个点相乘，对于每个通道上，用的f值是一样的，可以理解为spatial attention。</p>
<p><strong>[DANet - CVPR2019] Dual Attention Network for Scene Segmentation</strong></p>
<p>-Introduces a self-attention mechanism to capture features dependencies in the spatial and channel dimensions</p>
<p><img src="/2019/03/28/segmentation-paper/23.png" alt=""><br><img src="/2019/03/28/segmentation-paper/24.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/25.png" alt=""></p>
<p><strong>[CCNet - ICCV2019] CCNet: Criss-Cross Attention for Semantic Segmentation</strong></p>
<p>-The current no-local operation, can be alternatively replaced by two consecutive criss-cross operations, in which each one only has sparse connections (H + W - 1) for each position in the feature maps. By serially stacking two criss-cross attention modules, it can collect contextual information from all pixels. The decomposition greatly reduce the complexity in time and space from O((HxW)x(HxW)) to O((HxW)x(H +W - 1)).</p>
<p><img src="/2019/03/28/segmentation-paper/19.png" alt=""><br><img src="/2019/03/28/segmentation-paper/20.png" alt=""></p>
<p>-The details of criss-cross attention module:<br><img src="/2019/03/28/segmentation-paper/21.png" alt=""></p>
<p><img src="/2019/03/28/segmentation-paper/22.png" alt=""></p>
<p>-</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/28/super-resolution-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/28/super-resolution-paper/" itemprop="url">super-resolution-paper</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-28T09:33:55+08:00">
                2019-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="PSNR-Oriented-Approach"><a href="#PSNR-Oriented-Approach" class="headerlink" title="PSNR Oriented Approach"></a>PSNR Oriented Approach</h1><p><strong>[SRCNN - ECCV2014] Learning a Deep Convolutional Network for Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/1.png" alt=""></p>
<p>-The training data set is synthesized by extracting nonoverlapping dense patches of size 32x32 from the HR images. The LR input patches are first downsampled and then upsampled using bicubic interpolation having the same size as the high-resolution output image.</p>
<p>-MSE loss</p>
<hr>
<p><strong>[VDSR - CVPR2016] Accurate Image Super-Resolution Using Very Deep Convolutional Networks</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/2.png" alt=""></p>
<p>-VGG</p>
<p>-To speed-up the training: (1) learn a residual mapping that generates the difference between the HR and LR image instead of directly generating a HR image. (2) gradients are clipped with in the range [-θ, θ]. These allow very high learning rates.</p>
<hr>
<p><strong>[ESPCN - CVPR2016] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/4.png" alt=""></p>
<p>-Propose an efficient sub-pixel convolution layer to learn the upscaling operation for image and video super-resolution.</p>
<p>-Sub-pixel convolution : <a href="https://blog.csdn.net/bbbeoy/article/details/81085652" target="_blank" rel="noopener">https://blog.csdn.net/bbbeoy/article/details/81085652</a><br>First, use a convolution outputs H x W x Crr. Second, use periodic shuffling to rearange it to Hr x Wr x C </p>
<hr>
<p><strong>[DRCN - CVPR2016] Deeply-Recursive Convolutional Network for Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/13.png" alt=""></p>
<p>-Supervise all recursions in order to alleviate the effect of vanishing/exploding gradients.</p>
<p>-Add a layer skip from input to the reconstruction net.</p>
<hr>
<p><strong>[FSRCNN - ECCV2016] Accelerating the Super-Resolution Convolutional Neural Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/3.png" alt=""></p>
<p>-Use deconvolution as a post-upsamling step instead of upsampling the original LR image as a pre-processing step.</p>
<p>-Use PReLU instead of ReLU.</p>
<p>-Use the 91-image dataset [1] with another 100 images collected from the internet. Data augmentation such as rotation, flipping, and scaling is also employed to increase the number<br>of images by 19 times.<br><em>[1] J. Yang, J.Wright, T. S. Huang, and Y. Ma, “Image super-resolution via sparse representation,” TIP, 2010.</em></p>
<hr>
<p><strong>[RED-Net - NIPS2016] Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections</strong></p>
<hr>
<p><strong>[LapSRN - CVPR2017] Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/19.png" alt=""></p>
<p>-Progressively predicts residual images at log2(S) levels where S is the scale factor.</p>
<p>-Loss function:<br><img src="/2019/03/28/super-resolution-paper/20.png" alt=""></p>
<p>-3x3conv, 4x4deconv, lrelu.</p>
<hr>
<p><strong>[DRRN - CVPR2017] Image Super-Resolution via Deep Recursive Residual Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/14.png" alt=""></p>
<p>-Global and local residual learning</p>
<p>-Recursive block consisting of several residual units, and the weight set is shared among these residual units.</p>
<p>-For one block,  a multi-path structure is used and all the residual units share the same input for the identity branch.</p>
<hr>
<p><strong>[EDSR - CVPR2017W] Enhanced Deep Residual Networks for Single Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/5.png" alt=""></p>
<p>-Modify SRResNet:</p>
<p>(1) Remove Batch Normalization layers (from each residual block) and ReLU activation (outside residual blocks). Since batch normalization layers normalize the features, they get rid of range flexibility from networks by normalizing the features. Furthermore, GPU memory usage is also sufficiently reduced since the batch normalization layers consume the same amount of memory as the preceding convolutional layers. Consequently, we can build up a larger model.</p>
<p><img src="/2019/03/28/super-resolution-paper/6.png" alt=""></p>
<p>(2) Increasing F(the number of feature channels) instead of B(thenumber of layers) can maximize the model capacity when considering limited computational resources. Use residual scaling to stabilize the training procedure. In EDSR, set B = 32, F = 256, scaling factor=0.1.</p>
<p>-When training our model for upsampling factor x3 and x4, we initialize the model parameters with pre-trained x2 network.</p>
<p>-MDSR (Multi scale model) (B = 80 and F = 64)</p>
<p><img src="/2019/03/28/super-resolution-paper/7.png" alt=""></p>
<p>-L1 loss provides better convergence than L2.</p>
<hr>
<p><strong>[BTSRN - CVPR2017W] Balanced Two-Stage Residual Networks for Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/11.png" alt=""></p>
<p>-Only 10 residual blocks to ensure the efficiency. (6 for lr stage, 4 for hr stage)</p>
<p><img src="/2019/03/28/super-resolution-paper/12.png" alt=""></p>
<p>-For the up-sampling layers, the element sum of nearest neighbor up-sampling and deconvolution is employed. To reduce the artifacts, the stride and size of kernels are equal to scaling factor for x2 and x3, and two x2 up-sampling are applied for x4 scaling.</p>
<p>-Batch normalization is not suitable for super-resolution task. Because super-resolution is a regressing task, the target outputs are highly correlated to inputs first order statistics, while batch normalization makes the networks invariant to data re-centering and re-scaling.</p>
<p>-Predict the residual images.  L2 loss.</p>
<hr>
<p><strong>[SelNet - CVPR2017W] A Deep Convolutional Neural Network with Selection Units for Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/31.png" alt=""></p>
<hr>
<p><strong>[MemNet - ICCV2017] MemNet: A Persistent Memory Network for Image Restoration</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/17.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/16.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/15.png" alt=""></p>
<p>-Gate unit: 1x1conv</p>
<hr>
<p><strong>[SRDenseNet - ICCV2017] Image Super-Resolution Using Dense Skip Connections</strong><br><img src="/2019/03/28/super-resolution-paper/18.png" alt=""></p>
<hr>
<p><strong>[RDN - CVPR2018] Residual Dense Network for Image Super-Resolution</strong></p>
<p><em>加一点MemNet，加一点SRDenseNet</em></p>
<p><img src="/2019/03/28/super-resolution-paper/21.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/22.png" alt=""></p>
<p>-l1 loss which has been demonstrated to be more powerful for performance and convergence.</p>
<hr>
<p><strong>[DBPN - CVPR2018] Deep Back-Projection Networks For Super-Resolution</strong></p>
<p>-DBPN:<br><img src="/2019/03/28/super-resolution-paper/23.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/24.png" alt=""></p>
<p>-D-DBPN:<br><img src="/2019/03/28/super-resolution-paper/25.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/26.png" alt=""></p>
<p>-Avoid dropout and batch norm, use 1 x 1 convolution layer as feature pooling and dimensional<br>reduction instead.</p>
<p>-The projection unit uses large sized filters such as 8 x 8 and 12 x 12. In other existing networks, the use of largesized filter is avoided because it slows down the convergence speed and might produce sub-optimal results. However, iterative utilization of our projection units enables the network to suppress this limitation and to perform better performance on large scaling factor even with shallow net works.</p>
<hr>
<p><strong>[IDN - CVPR2018] Fast and Accurate Single Image Super-Resolution via Information Distillation Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/27.png" alt=""></p>
<p>-Enhancement unit (each of convs is followed by LReLU)<br><img src="/2019/03/28/super-resolution-paper/28.png" alt=""></p>
<p>-Compression unit: 1x1conv</p>
<p>-First train the network with MAE loss and then fine-tune it by MSE loss</p>
<hr>
<p><strong>[CARN - ECCV2018] Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network</strong></p>
<p><em>其实就是把DRRN里resnet的连线改为densenet的连线，加了些1x1conv</em></p>
<p>-Global cascade:</p>
<p><img src="/2019/03/28/super-resolution-paper/8.png" alt=""></p>
<p>-Cascading block, local cascade and efficient residual (residual-E) block.  And to further reduce the parameters, make the parameters of the Cascading blocks shared, effectively making the blocks recursive.<br><img src="/2019/03/28/super-resolution-paper/9.png" alt=""></p>
<p>-Cascading on both the local and global levels has two advantages: 1) The model<br>incorporates features from multiple layers, which allows learning multi-level rep-<br>resentations. 2) Multi-level cascading connection behaves as multi-level shortcut<br>connections that quickly propagate information from lower to higher layers (and<br>vice-versa, in case of back-propagation).</p>
<p><img src="/2019/03/28/super-resolution-paper/10.png" alt=""></p>
<hr>
<p><strong>[RCAN - ECCV2018] Image Super-Resolution Using Very Deep Residual Channel Attention Networks</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/29.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/30.png" alt=""></p>
<hr>
<p><strong>[SRRAM - arXiv1811] RAM: Residual Attention Module for Single Image Super-Resolution</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/32.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/33.png" alt=""></p>
<p>-Channel Attention: since SR ultimately aims at restoring high-frequency components of images,<br>it is more reasonable for attention maps to be determined using high-frequency statistics about the channels. To this end, we choose to use the variance rather than the average for the pooling method</p>
<p>-Spatial Attention: use depth-wise convolution</p>
<h1 id="GAN-based-Approach"><a href="#GAN-based-Approach" class="headerlink" title="GAN based Approach"></a>GAN based Approach</h1><p><strong>[SRGAN - CVPR2017] Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/34.png" alt=""></p>
<p>-Perceptual loss function<br><img src="/2019/03/28/super-resolution-paper/35.png" alt=""></p>
<p>-content loss<br>With φ<sub>ij</sub> we indicate the feature map obtained by the j-th convolution (after activation) before the i-th maxpooling layer within the VGG19 network.<br><img src="/2019/03/28/super-resolution-paper/36.png" alt=""></p>
<p>-adverarial loss<br><img src="/2019/03/28/super-resolution-paper/37.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/38.png" alt=""></p>
<hr>
<p><strong>[EhanceNet - ICCV2017] EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis</strong></p>
<hr>
<p><strong>[SRFeat - ECCV2018] SRFeat: Single Image Super-Resolution with Feature Discrimination</strong></p>
<p>-While GAN-based SISR methods show dramatic improvements over previous approaches in terms of perceptual quality, they often tend to produce less meaningful high-frequency noise in super-resolved images. We argue that this is because the most dominant difference between super-resolved images and real HR images is high-frequency information, where super-resolved images obtained by minimizing pixel-wise errors lack high-frequency details. The simplest way for a discriminator to distinguish super-resolved images from real HR images could be simply inspecting the presence of high-frequency components in a given im- age, and the simplest way for a generator to fool the discriminator would be to put arbitrary high-frequency noise into result images.</p>
<p><img src="/2019/03/28/super-resolution-paper/39.png" alt=""></p>
<p>-First pre-train using MSE loss, then go adversarial training.</p>
<p><img src="/2019/03/28/super-resolution-paper/40.png" alt=""></p>
<p>-L<sub>p</sub>: perceptual Similarity Loss.<br>L<sup>i</sup><sub>a</sub> : image gan loss.<br><img src="/2019/03/28/super-resolution-paper/42.png" alt=""><br>L<sup>f</sup><sub>a</sub> : feature gan loss.<br><img src="/2019/03/28/super-resolution-paper/41.png" alt=""></p>
<hr>
<p><strong>[ESRGAN - ECCV2018W] Enhanced Super-Resolution Generative Adversarial Networks</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/43.png" alt=""><br><img src="/2019/03/28/super-resolution-paper/44.png" alt=""></p>
<p>-Net Architecture:<br>Like EDSR: When the statistics of training and testing datasets differ a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. We empirically observe that BN layers are more likely to bring artifacts when the network is deeper and trained under a GAN framework.</p>
<p>Residual leaning.<br>Smaller initialization</p>
<p>-Relativistic Discriminator<br>A relativistic discriminator tries to predict the probability that a real image x<sub>r</sub> is relatively more realistic than a fake one x<sub>f</sub><br><img src="/2019/03/28/super-resolution-paper/45.png" alt=""><br>where E[ ] represents the operation of taking average for all fake or real data in the mini-batch.</p>
<p>The discriminator loss is then defined as:<br><img src="/2019/03/28/super-resolution-paper/46.png" alt=""><br>The adversarial loss for generator is in a symmetrical form:<br><img src="/2019/03/28/super-resolution-paper/47.png" alt=""></p>
<p>It is observed that the adversarial loss for generator contains both xr and xf . Therefore, our generator benefits from the gradients from both generated data and real data in adversarial training, while in SRGAN only generated part takes effect. The experiment shows this modification of discriminator helps to learn sharper edges and more detailed textures.</p>
<p>-Perceptual Loss:<br>Develop a more effective perceptual loss by constraining on features before activation rather than after activation. Two reasons: first, the activated features are very sparse; second, using features after activation also causes inconsistent reconstructed brightness</p>
<p>-total loss for the generator<br><img src="/2019/03/28/super-resolution-paper/48.png" alt=""></p>
<p>-Network Interpolation<br>To remove unpleasant noise in GAN-based methods while maintain a good perceptual quality<br><img src="/2019/03/28/super-resolution-paper/49.png" alt=""></p>
<p><strong>[RankSRGAN - ICCV2019] RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution</strong></p>
<h1 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h1><p>Four steps: feature extraction, alignment, fusion, and reconstruction.</p>
<p><strong>[VESPCN - CVPR2017] Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation</strong></p>
<p><strong>[SPMC - ICCV2017] Detail-revealing Deep Video Super-resolution </strong></p>
<p><strong>[FRVSR - CVPR2018] Frame-Recurrent Video Super-Resolution</strong></p>
<p><strong>[DUF - CVPR2018] Deep Video Super-Resolution Network Using Dynamic Upsampling FiltersWithout Explicit Motion Compensation</strong></p>
<p><img src="/2019/03/28/super-resolution-paper/50.png" alt=""></p>
<p>-<em>Dynamic Upsampling Filters</em><br><img src="/2019/03/28/super-resolution-paper/51.png" alt=""></p>
<p>First, a set of input LR frames {Xt−N:t+N} (7 frames in our network: N = 3) is fed into the dynamic filter generation network. The trained network outputs a set of r<sup>2</sup>HW upsampling filters Ft of a certain size (5 × 5 in our network).(F的大小为: 5 x 5 x r<sup>2</sup>HW, 这里的r是scale factor)</p>
<p>然后对于原图中的一点，用周围5x5的点与F中5x5xr<sup>2</sup>依次相乘，得到rxr个点，从而upsampling了r倍。</p>
<p>-<em>Residual Learning</em><br>The result after applying the dynamic upsampling filters alone lacks sharpness as it is still a weighted sum of input pixels.To address this, we additionally estimate a residual image to increase high frequency details</p>
<p>-<em>Network Design</em><br>3D convolutional layers / filter and residual generation network are designed to share most of the weights</p>
<p><strong>[EDVR - CVPRW2019] EDVR: Video Restoration with Enhanced Deformable Convolutional Networks</strong></p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/28/linux/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/28/linux/" itemprop="url">linux</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-28T09:04:58+08:00">
                2019-03-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><table>
<thead>
<tr>
<th style="text-align:left"></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">vncserver -geometry 1920x1080</td>
<td>创建一个1920x1080的vnc</td>
</tr>
<tr>
<td style="text-align:left">vncserver -kill :1</td>
<td>删除ID为1的vnc</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">mkdir XXX</td>
<td>建文件夹</td>
</tr>
<tr>
<td style="text-align:left">mkdir -p AAA/BBB</td>
<td>建BBB，若AAA不存在则先建AAA</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">cd XX</td>
<td>到某个目录</td>
</tr>
<tr>
<td style="text-align:left">cd ..</td>
<td>返回上一级目录</td>
</tr>
<tr>
<td style="text-align:left">cd ~</td>
<td>到home目录</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">pip intsall</td>
<td>装某一个python的库</td>
</tr>
<tr>
<td style="text-align:left">nvidia-smi</td>
<td>查看gpu使用情况</td>
</tr>
<tr>
<td style="text-align:left">./XX.sh</td>
<td>运行某个.sh文件</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">rm -rf XXX</td>
<td>删除整个文件夹</td>
</tr>
<tr>
<td style="text-align:left">rm XXX</td>
<td>删除一个文件</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">cp -a XX(source) YY(destination)</td>
<td>将整个XX复制到YY</td>
</tr>
<tr>
<td style="text-align:left">mv XX YY</td>
<td>将XX移动到YY</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">df -h</td>
<td>查看系统中文件的使用情况</td>
</tr>
<tr>
<td style="text-align:left">du -sh *</td>
<td>查看当前目录下各个文件及目录占用空间大小</td>
</tr>
<tr>
<td style="text-align:left">du -sh FILE/</td>
<td>查看目录文件总大小</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">tar</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<hr>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><strong>zjuvpn安装</strong></p>
<p><a href="https://www.cc98.org/topic/2323871" target="_blank" rel="noopener">https://www.cc98.org/topic/2323871</a></p>
<p>1.删除原来的xl2tpd包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg --purge xl2tpd</span><br></pre></td></tr></table></figure>
<p>2.下载：<br><a href="https://pan.baidu.com/s/1eRNQwng#list/path=%2F" target="_blank" rel="noopener">https://pan.baidu.com/s/1eRNQwng#list/path=%2F</a></p>
<p>3.安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i xl2tpd_1.1.12-zju2_i386.deb</span><br></pre></td></tr></table></figure>
<p>报错：没有iproute 所以要先装一个：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install iproute</span><br></pre></td></tr></table></figure></p>
<p>4.配置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vpn-connect -c</span><br></pre></td></tr></table></figure>
<p>按照提示操作, 注意用户名 学号后面要加@a</p>
<p>5.连接:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vpn-connect</span><br></pre></td></tr></table></figure></p>
<p>6.断开:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vpn-connect -d</span><br></pre></td></tr></table></figure>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/25/high-dynamic-range-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/25/high-dynamic-range-paper/" itemprop="url">High Dynamic Range Paper</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-25T20:57:15+08:00">
                2019-03-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Using-a-series-of-low-dynamic-range-images-at-different-exposure"><a href="#Using-a-series-of-low-dynamic-range-images-at-different-exposure" class="headerlink" title="Using a series of low dynamic range images at different exposure"></a>Using a series of low dynamic range images at different exposure</h1><p>Generally, this problem can be broken down into two stages: 1) aligning the input LDR images and 2) merging the aligned images into an HDR image.</p>
<p>This method produces spectacular images for tripod mounted cameras and static scenes, but generates results with ghosting artifacts when the scene is dynamic or the camera is hand-held.</p>
<hr>
<h2 id="Deep-High-Dynamic-Range-Imaging-of-Dynamic-Scenes-SIGGRAPH2017"><a href="#Deep-High-Dynamic-Range-Imaging-of-Dynamic-Scenes-SIGGRAPH2017" class="headerlink" title="Deep High Dynamic Range Imaging of Dynamic Scenes - SIGGRAPH2017"></a>Deep High Dynamic Range Imaging of Dynamic Scenes - SIGGRAPH2017</h2><p>-the artifacts of the alignment can be significantly reduced during merging</p>
<p>-<em>Preprocessing the Input LDR Images:</em> </p>
<p>If the LDR images are not in the RAW format, we first linearize them using the camera response function (CRF), then apply gamma correction (γ = 2.2). The gamma correction basically maps the images into a domain that is closer to what we perceive with our eyes.</p>
<p>-<em>Alignment:</em> </p>
<p>Produce aligned images by registering the images with low (Z1) and high (Z3) exposures to the reference image Z2 using tranditional method. (optical flow)</p>
<p>-<em>HDR Merge:</em></p>
<p>1)<em>Model</em>:<br><img src="/2019/03/25/high-dynamic-range-paper/1.png" alt=""><br><img src="/2019/03/25/high-dynamic-range-paper/2.png" alt=""></p>
<p>2)<em>Loss Function</em>:</p>
<p>Since HDR images are usually displayed after <em>tonemapping</em>, we propose to compute our loss function between the tonemapped estimated and ground truth HDR images. We propose to use μ-law, a commonly-used range compressor in audio processing, which is differentiable.<br><img src="/2019/03/25/high-dynamic-range-paper/3.png" alt=""></p>
<p>we train the learning system by minimizing the L2 distance of the tonemapped estimated and<br>ground truth HDR images defined as:<br><img src="/2019/03/25/high-dynamic-range-paper/4.png" alt=""></p>
<hr>
<h2 id="Deep-High-Dynamic-Range-Imaging-with-Large-Foreground-Motions-ECCV2018"><a href="#Deep-High-Dynamic-Range-Imaging-with-Large-Foreground-Motions-ECCV2018" class="headerlink" title="Deep High Dynamic Range Imaging with Large Foreground Motions - ECCV2018"></a>Deep High Dynamic Range Imaging with Large Foreground Motions - ECCV2018</h2><p>-CNNs have been demonstrated to have the ability to learn misalignment and hallucinate missing details</p>
<p>-Three advantage: 1) trained end-to-end without optical flow alignment. 2)can hallucinate plausible details that are totally missing or their presence is extremely weak in all LDR inputs. 3) the same framework can be easily extended to more LDR inputs, and possibly with any specified reference image.</p>
<p>-<em>Network Architecture</em><br><img src="/2019/03/25/high-dynamic-range-paper/11.png" alt=""><br>We separate the first two layers as encoders for each exposure inputs. After extracting the features, the network learns to merge them, mostly in the middle layers, and to decode them into an HDR output, mostly in the last few layers.</p>
<p>-<em>Processing Pipeline and Loss Function</em></p>
<p>Given a stack of LDR images, if they are not in RAW format, we first linearize the images using the estimated inverse of Camera Response Function (CRF), which is often referred to as radiometric calibration. We then apply gamma correction to produce the input to our system.</p>
<p>We first map LDRs to H = {H1;H2;H3} in the HDR domain, using simple gamma encoding:<br><img src="/2019/03/25/high-dynamic-range-paper/12.png" alt=""><br>where ti is the exposure time of image Ii. We then concatenate I and H channel-wise into a 6-channel input and feed it directly to the network. The LDRs facilitate the detection of misalignments and saturation, while the exposure-adjusted HDRs improve the robustness of the network across LDRs with various exposure levels.</p>
<p>Tonemapping function and loss function are the same as the previous paper.<br><img src="/2019/03/25/high-dynamic-range-paper/13.png" alt=""></p>
<p>-<em>Data Preparation</em></p>
<p>First align the background using simple homography transformation by <em>homography transformation</em>. Without it, we found that our network tends to produce blurry edges where background is largely misaligned.</p>
<p>Crop the images into 256x256 patches with a stride of 64. To keep the training focused on foreground motions, we detect large motion patches by thresholding the structural similarity between different exposure shots, and replicate these patches in the training set.</p>
<h1 id="Using-single-low-dynamic-range-image"><a href="#Using-single-low-dynamic-range-image" class="headerlink" title="Using single low dynamic range image"></a>Using single low dynamic range image</h1><p>One intrinsic limitation of this approach is the total reliance on one single input LDR image, which often fails in highly contrastive scenes due to large-scale saturation.</p>
<h2 id="HDR-image-reconstruction-from-a-single-exposure-using-deep-CNNs-1710"><a href="#HDR-image-reconstruction-from-a-single-exposure-using-deep-CNNs-1710" class="headerlink" title="HDR image reconstruction from a single exposure using deep CNNs - 1710"></a>HDR image reconstruction from a single exposure using deep CNNs - 1710</h2><p>-Estimating missing information in bright image parts, such as highlights, lost due to saturation of the camera sensor<br><img src="/2019/03/25/high-dynamic-range-paper/21.png" alt=""></p>
<hr>
<h2 id="ExpandNet-A-Deep-Convolutional-Neural-Network-for-High-Dynamic-Range-Expansion-from-Low-Dynamic-Range-Content-EUROGRAPHICS-2018"><a href="#ExpandNet-A-Deep-Convolutional-Neural-Network-for-High-Dynamic-Range-Expansion-from-Low-Dynamic-Range-Content-EUROGRAPHICS-2018" class="headerlink" title="ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content - EUROGRAPHICS 2018"></a>ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range Expansion from Low Dynamic Range Content - EUROGRAPHICS 2018</h2><p>-Designed to avoid upsampling of downsampled features, in an attempt to reduce blocking and/or haloing artefacts that may arise from more straightforward approaches.</p>
<p>-It is argued that upsampling, especially the frequently used deconvolutional layers, cause checkerboard artefacts. Furthermore, upsampling may cause unwanted information bleeding in areas where context is missing, for example large overexposed areas.</p>
<p>-The local branch handling local detail, the dilation branch for medium level detail, and a global branch accounting for higher level image-wide features<br><img src="/2019/03/25/high-dynamic-range-paper/31.png" alt=""></p>
<p>-<em>Loss Function</em></p>
<p>The L1 distance is chosen for this problem since the more frequently used L2 distance was found to cause blurry results for images. An additional cosine similarity term is added to ensure color correctness of the RGB vectors of each pixel.<br><img src="/2019/03/25/high-dynamic-range-paper/32.png" alt=""></p>
<p>Cosine similarity measures how close two vectors are by comparing the angle between them, not taking magnitude into account. For the context of this work, it ensures that each pixel points in the same direction of the three dimensional RGB space. It provides improved color stability, especially for low luminance values, which are frequent in HDR images, since slight variations in any of the RGB components of these low values do not contribute much to the L1 loss, but they may however cause noticeable color shifts.</p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p><strong>proposed by Kalantari(Deep High Dynamic Range Imaging of Dynamic Scenes):</strong></p>
<p>To generate the ground truth HDR image, we capture a static set by asking a subject to stay still and taking three images with different exposures on a tripod.</p>
<p>Next, we capture a dynamic set to use as our input by asking the subject to move and taking three bracketed exposure images either by holding the camera (to simulate camera motion) or on a tripod.</p>
<p>Capture all the images in RAW format with a resolution of 5760 × 3840 and using a Canon EOS-5D Mark III camera. Downsample all the images (including the dynamic set) to the resolution of 1500 × 1000.</p>
<p>Use color channel swapping and geometric transformation (rotating 90 degrees and flipping) with 6 and 8 different combinations, respectively. This process produces a total of 48 different combinations of data augmentation, from which we randomly choose 10 combinations to augment each training scene. Our data augmentation process increases the number of training scenes from 74 to 740.</p>
<p>Finally, since training on full images is slow, we break down the training images into overlapping patches of size 40 × 40 with a stride of 20. This process produces a set of training patches consisting of the aligned patches in the LDR and HDR domains as well as their corresponding ground truth HDR patches. We then select the training patches where more than 50 percent of their reference patch is under/over-exposed, which results in around 1,000,000 selected patches. This selection is performed to put the main focus of the networks on the challenging regions.</p>
<p><strong>Described in DeepHDR</strong></p>
<p>The dataset was split into 74 training examples and 15 testing examples. crop the images into 256x256 patches with a stride of 64, which produces around 19000 patches. We then perform data augmentation (flipping and rotation), further increasing the training data by 8 times.</p>
<p>In fact, a large portion of these patches contain only background regions, and exhibit little foreground motions. To keep the training focused on foreground motions, we detect large motion patches by thresholding the structural similarity<br>between different exposure shots, and replicate these patches in the training set.</p>
<p><strong>ExpandNet</strong></p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>Sen:  Robust Patch-Based HDR Reconstruction of Dynamic Scenes. ACM TOG 31(6), 203:1-203:11 (2012)</p>
<p>Hu: HDR Deghosting: How to deal with Saturation? In: IEEE CVPR (2013)</p>
<p>Kalantari: Deep High Dynamic Range Imaging of Dynamic Scenes. ACM TOG 36(4) (2017)</p>
<p>HDRCNN: HDR image reconstruction from a single exposure using deep cnns. ACM TOG 36(6) (2017)</p>
<p>Ours: Deep High Dynamic Range Imaging with Large Foreground Motions</p>
<h2 id="Running-Time"><a href="#Running-Time" class="headerlink" title="Running Time"></a>Running Time</h2><p>PC with i7-4790K (4.0GHz) and 32GB RAM, 3 LDR images of size 896x1408 as input.</p>
<p><img src="/2019/03/25/high-dynamic-range-paper/41.png" alt=""></p>
<p>When run with GPU (Titan X Pascal), our Unet and ResNet take 0.225s and 0.239s respectively.</p>
<h2 id="Quantitative-Comparison"><a href="#Quantitative-Comparison" class="headerlink" title="Quantitative Comparison"></a>Quantitative Comparison</h2><p><img src="/2019/03/25/high-dynamic-range-paper/42.png" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Pan Sicheng</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Pan Sicheng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
