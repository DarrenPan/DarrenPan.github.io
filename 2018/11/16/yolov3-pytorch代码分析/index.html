<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,Deep Learning,Computer Vision,Object Detection," />










<meta name="description" content="GitHub：https://github.com/ayooshkathuria/pytorch-yolo-v3 Tutorial： Implement YOLO v3 from scratch   1. Creating the layers of the network architectureConfiguration FileThe configuration  file here des">
<meta name="keywords" content="Machine Learning,Deep Learning,Computer Vision,Object Detection">
<meta property="og:type" content="article">
<meta property="og:title" content="yolov3-pytorch代码分析">
<meta property="og:url" content="http://yoursite.com/2018/11/16/yolov3-pytorch代码分析/index.html">
<meta property="og:site_name" content="Good Good Study">
<meta property="og:description" content="GitHub：https://github.com/ayooshkathuria/pytorch-yolo-v3 Tutorial： Implement YOLO v3 from scratch   1. Creating the layers of the network architectureConfiguration FileThe configuration  file here des">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2018/11/16/yolov3-pytorch代码分析/1.png">
<meta property="og:image" content="http://yoursite.com/2018/11/16/yolov3-pytorch代码分析/32.png">
<meta property="og:image" content="http://yoursite.com/2018/11/16/yolov3-pytorch代码分析/4.png">
<meta property="og:updated_time" content="2019-01-25T06:55:41.315Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="yolov3-pytorch代码分析">
<meta name="twitter:description" content="GitHub：https://github.com/ayooshkathuria/pytorch-yolo-v3 Tutorial： Implement YOLO v3 from scratch   1. Creating the layers of the network architectureConfiguration FileThe configuration  file here des">
<meta name="twitter:image" content="http://yoursite.com/2018/11/16/yolov3-pytorch代码分析/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/11/16/yolov3-pytorch代码分析/"/>





  <title>yolov3-pytorch代码分析 | Good Good Study</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Good Good Study</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/16/yolov3-pytorch代码分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pan Sicheng">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Good Good Study">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">yolov3-pytorch代码分析</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-16T10:18:03+08:00">
                2018-11-16
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>GitHub：<a href="https://github.com/ayooshkathuria/pytorch-yolo-v3" target="_blank" rel="noopener">https://github.com/ayooshkathuria/pytorch-yolo-v3</a></p>
<p>Tutorial： <a href="https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/" target="_blank" rel="noopener">Implement YOLO v3 from scratch</a> </p>
<hr>
<h1 id="1-Creating-the-layers-of-the-network-architecture"><a href="#1-Creating-the-layers-of-the-network-architecture" class="headerlink" title="1. Creating the layers of the network architecture"></a>1. Creating the layers of the network architecture</h1><h2 id="Configuration-File"><a href="#Configuration-File" class="headerlink" title="Configuration File"></a>Configuration File</h2><p>The configuration  file <a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg" target="_blank" rel="noopener">here</a> describes the layout of the network, block by block. You can also see the full architecture Darknet-53 in the folloing diagram.</p>
<p><img src="/2018/11/16/yolov3-pytorch代码分析/1.png" alt=""></p>
<p>There are 5 types of layers that are used in YOLO:</p>
<p><strong><em>Convolutional</em></strong></p>
<p><strong><em>Shortcut（Residual）</em></strong>: 参数<code>from:-3</code>表示shortcut的output等于倒数第三层的output与前一层output相加</p>
<p><strong><em>Upsample</em></strong>: YOLO3采用了类似FPN的结构，在三个尺度上进行预测，故要做两次unsample</p>
<p><strong><em>Route</em></strong>：<br>When layers attribute has only one value, it outputs the feature maps of the layer indexed by the value. In our example, it is -4, so the layer will output feature map from the 4th layer backwards from the Route layer.</p>
<p>When layers has two values, it returns the concatenated feature maps of the layers indexed by it’s values. In our example it is -1, 61, and the layer will output feature maps from the previous layer (-1) and the 61st layer, concatenated along the depth dimension.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[route]</span><br><span class="line">layers = -4</span><br><span class="line"></span><br><span class="line">[route]</span><br><span class="line">layers = -1, 61</span><br></pre></td></tr></table></figure></p>
<p><strong><em>YOLO</em></strong>：<br>The anchors describes 9 anchors, but only the anchors which are indexed by attributes of the mask tag are used. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[yolo]</span><br><span class="line">mask = 0,1,2</span><br><span class="line">anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326</span><br><span class="line">classes=80</span><br><span class="line">num=9</span><br><span class="line">jitter=.3</span><br><span class="line">ignore_thresh = .5</span><br><span class="line">truth_thresh = 1</span><br><span class="line">random=1</span><br></pre></td></tr></table></figure>
<h2 id="Parsing-the-configuration-file"><a href="#Parsing-the-configuration-file" class="headerlink" title="Parsing the configuration file"></a>Parsing the configuration file</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_cfg</span><span class="params">(cfgfile)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Takes a configuration file</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns a list of blocks. Each blocks describes a block in the neural</span></span><br><span class="line"><span class="string">    network to be built. Block is represented as a dictionary in the list</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure>
<h2 id="Creating-the-building-blocks"><a href="#Creating-the-building-blocks" class="headerlink" title="Creating the building blocks"></a>Creating the building blocks</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_modules</span><span class="params">(blocks)</span>:</span></span><br><span class="line">    net_info = blocks[<span class="number">0</span>]     <span class="comment">#Captures the information about the input and pre-processing    </span></span><br><span class="line">    module_list = nn.ModuleList()</span><br><span class="line">    prev_filters = <span class="number">3</span></span><br><span class="line">    output_filters = []</span><br></pre></td></tr></table></figure>
<p>Our function will return a <code>nn.ModuleList</code> .</p>
<p><code>prev_filters</code> is used to keep track of number of filters in the layer on which the convolutional layer is being applied. </p>
<p><code>output_filters</code> is used to store the number of output filters of each block, for route layer need.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for x in blocks:</span><br><span class="line">    module = nn.Sequential()</span><br></pre></td></tr></table></figure>
<p><code>nn.Sequential</code> class is used to sequentially execute a number of nn.Module objects. We use it’s <code>add_module</code> function to string together all layers.</p>
<p><strong>for conv layer and unsample layer</strong>:(PyTorch has provided pre-built layers)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Add the convolutional layer</span></span><br><span class="line">conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)</span><br><span class="line">module.add_module(<span class="string">"conv_&#123;0&#125;"</span>.format(index), conv)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Add the Batch Norm Layer</span></span><br><span class="line">bn = nn.BatchNorm2d(filters)</span><br><span class="line">module.add_module(<span class="string">"batch_norm_&#123;0&#125;"</span>.format(index), bn)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Check the activation. </span></span><br><span class="line"><span class="comment">#It is either Linear or a Leaky ReLU for YOLO</span></span><br><span class="line">activn = nn.LeakyReLU(<span class="number">0.1</span>, inplace = <span class="keyword">True</span>)</span><br><span class="line">module.add_module(<span class="string">"leaky_&#123;0&#125;"</span>.format(index), activn)</span><br><span class="line"></span><br><span class="line"><span class="comment">#If it's an upsampling layer</span></span><br><span class="line"><span class="comment">#We use Bilinear2dUpsampling</span></span><br><span class="line">upsample = nn.Upsample(scale_factor = <span class="number">2</span>, mode = <span class="string">"bilinear"</span>)</span><br><span class="line">module.add_module(<span class="string">"upsample_&#123;&#125;"</span>.format(index), upsample)</span><br></pre></td></tr></table></figure>
<p><strong>for Route Layer / Shortcut Layers</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shortcut = EmptyLayer()</span><br><span class="line">module.add_module(<span class="string">"shortcut_&#123;&#125;"</span>.format(index), shortcut)</span><br><span class="line"></span><br><span class="line">route = EmptyLayer()</span><br><span class="line">module.add_module(<span class="string">"route_&#123;0&#125;"</span>.format(index), route)</span><br></pre></td></tr></table></figure>
<p>The empty layer is defined as:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmptyLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(EmptyLayer, self).__init__()</span><br></pre></td></tr></table></figure></p>
<p>We use empty layer and  perform the concatenation directly in the<code>forward</code> function of the <code>nn.Module</code> object representing darknet.</p>
<p><strong>for yolo layers</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">detection = DetectionLayer(anchors)</span><br><span class="line">module.add_module(<span class="string">"Detection_&#123;&#125;"</span>.format(index), detection)</span><br></pre></td></tr></table></figure>
<p>We define a new layer<code>DetectionLayer</code> that holds the anchors used to detect bounding boxes.</p>
<p>The detection layer is defined as: (the <code>forward</code> function will be proposed later)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DetectionLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, anchors)</span>:</span></span><br><span class="line">        super(DetectionLayer, self).__init__()</span><br><span class="line">        self.anchors = anchors</span><br></pre></td></tr></table></figure></p>
<p>At the end of the loop for each block， we do some bookkeeping.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">module_list.append(module)</span><br><span class="line">prev_filters = filters</span><br><span class="line">output_filters.append(filters)</span><br><span class="line">index += <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>At the end of the function <code>create_modules</code>, we return a tuple containing the <code>net_info</code>, and <code>module_list</code>.</p>
<h1 id="2-Implementing-the-the-forward-pass-of-the-network"><a href="#2-Implementing-the-the-forward-pass-of-the-network" class="headerlink" title="2. Implementing the the forward pass of the network"></a>2. Implementing the the forward pass of the network</h1><h2 id="Defining-the-Network"><a href="#Defining-the-Network" class="headerlink" title="Defining the Network"></a>Defining the Network</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Darknet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cfgfile)</span>:</span></span><br><span class="line">        super(Darknet, self).__init__()</span><br><span class="line">        self.blocks = parse_cfg(cfgfile)</span><br><span class="line">        self.net_info, self.module_list = create_modules(self.blocks)</span><br></pre></td></tr></table></figure>
<h2 id="Implementing-the-forward-pass-of-the-network"><a href="#Implementing-the-forward-pass-of-the-network" class="headerlink" title="Implementing the forward pass of the network"></a>Implementing the forward pass of the network</h2><p><code>forward</code> serves two purposes. First, to calculate the output, and second, to transform the output detection feature maps in a way that it can be processed easier</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, CUDA)</span>:</span></span><br><span class="line">    modules = self.blocks[<span class="number">1</span>:]  <span class="comment"># the first element is net layer which is no use</span></span><br><span class="line">    outputs = &#123;&#125;   <span class="comment"># We cache the outputs for the route layer</span></span><br><span class="line">    </span><br><span class="line">    write = <span class="number">0</span>     <span class="comment">#This is explained a bit later</span></span><br><span class="line">	<span class="keyword">for</span> i, module <span class="keyword">in</span> enumerate(modules):        </span><br><span class="line">    	module_type = (module[<span class="string">"type"</span>])</span><br></pre></td></tr></table></figure>
<p>Since route and shortcut layers need output maps from previous layers, we cache the output feature maps of every layer in a dict <code>outputs</code>. The keys are the the indices of the layers, and the values are the feature maps.</p>
<h2 id="Convolutional-and-Upsample-Layers"><a href="#Convolutional-and-Upsample-Layers" class="headerlink" title="Convolutional and Upsample Layers"></a>Convolutional and Upsample Layers</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> module_type == <span class="string">"convolutional"</span> <span class="keyword">or</span> module_type == <span class="string">"upsample"</span>:</span><br><span class="line">    x = self.module_list[i](x)</span><br></pre></td></tr></table></figure>
<h2 id="Route-Layer-Shortcut-Layer"><a href="#Route-Layer-Shortcut-Layer" class="headerlink" title="Route Layer / Shortcut Layer"></a>Route Layer / Shortcut Layer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> module_type == <span class="string">"route"</span>:</span><br><span class="line">    layers = module[<span class="string">"layers"</span>]</span><br><span class="line">    layers = [int(a) <span class="keyword">for</span> a <span class="keyword">in</span> layers]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (layers[<span class="number">0</span>]) &gt; <span class="number">0</span>:</span><br><span class="line">        layers[<span class="number">0</span>] = layers[<span class="number">0</span>] - i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(layers) == <span class="number">1</span>:</span><br><span class="line">        x = outputs[i + (layers[<span class="number">0</span>])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> (layers[<span class="number">1</span>]) &gt; <span class="number">0</span>:</span><br><span class="line">            layers[<span class="number">1</span>] = layers[<span class="number">1</span>] - i</span><br><span class="line"></span><br><span class="line">        map1 = outputs[i + layers[<span class="number">0</span>]]</span><br><span class="line">        map2 = outputs[i + layers[<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">        x = torch.cat((map1, map2), <span class="number">1</span>) <span class="comment"># concatenate the feature maps along the depth</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">elif</span>  module_type == <span class="string">"shortcut"</span>:</span><br><span class="line">    from_ = int(module[<span class="string">"from"</span>])</span><br><span class="line">    x = outputs[i<span class="number">-1</span>] + outputs[i+from_]</span><br></pre></td></tr></table></figure>
<p>In PyTorch, input and output of a convolutional layer has the format B x C x H x W. The depth corresponding the the channel dimension.</p>
<h2 id="YOLO-Detection-Layer"><a href="#YOLO-Detection-Layer" class="headerlink" title="YOLO (Detection Layer)"></a>YOLO (Detection Layer)</h2><p>The output of YOLO is a convolutional feature map that contains the bounding box attributes along the depth of the feature map. </p>
<p>There are two problems. First, the attributes bounding boxes predicted by a cell are stacked one by one along each other,  this form is very inconvenient for output processing. Second, it would be nice to have to do these operations on a single tensor, rather than three separate tensors.</p>
<p>To remedy these problems, we will introduce the function <code>predict_transform</code> first.</p>
<h2 id="Transforming-the-output"><a href="#Transforming-the-output" class="headerlink" title="Transforming the output"></a>Transforming the output</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_transform</span><span class="params">(prediction, inp_dim, anchors, num_classes, CUDA = True)</span>:</span></span><br></pre></td></tr></table></figure>
<p>This function takes an detection feature map and turns it into a 2-D tensor, where each row of the tensor corresponds to attributes of a bounding box, in the following order .<br><img src="/2018/11/16/yolov3-pytorch代码分析/32.png" alt=""></p>
<p>The code to do above transformation is:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_size = prediction.size(<span class="number">0</span>)</span><br><span class="line">stride =  inp_dim // prediction.size(<span class="number">2</span>)</span><br><span class="line">grid_size = inp_dim // stride</span><br><span class="line">bbox_attrs = <span class="number">5</span> + num_classes</span><br><span class="line">num_anchors = len(anchors)</span><br><span class="line"></span><br><span class="line">prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)</span><br><span class="line">prediction = prediction.transpose(<span class="number">1</span>,<span class="number">2</span>).contiguous()</span><br><span class="line">prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)</span><br></pre></td></tr></table></figure></p>
<p>The dimensions of the anchors are in accordance to the <code>height</code> and <code>width</code> attributes of the <code>net</code> block. These attributes describe the dimensions of the input image, which is larger (by a factor of stride) than the detection map. Therefore, we must divide the anchors by the stride of the detection feature map.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">anchors = [(a[<span class="number">0</span>]/stride, a[<span class="number">1</span>]/stride) <span class="keyword">for</span> a <span class="keyword">in</span> anchors]</span><br></pre></td></tr></table></figure></p>
<p>Now, we need to transform our output according to the equations<br><img src="/2018/11/16/yolov3-pytorch代码分析/4.png" alt=""></p>
<p>First, sigmoid the x,y coordinates and the objectness score.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Sigmoid the  centre_X, centre_Y. and object confidencce</span></span><br><span class="line">prediction[:,:,<span class="number">0</span>] = torch.sigmoid(prediction[:,:,<span class="number">0</span>])</span><br><span class="line">prediction[:,:,<span class="number">1</span>] = torch.sigmoid(prediction[:,:,<span class="number">1</span>])</span><br><span class="line">prediction[:,:,<span class="number">4</span>] = torch.sigmoid(prediction[:,:,<span class="number">4</span>])</span><br></pre></td></tr></table></figure></p>
<p>Add the grid offsets to the center cordinates prediction.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Add the center offsets</span></span><br><span class="line">grid = np.arange(grid_size)</span><br><span class="line">a,b = np.meshgrid(grid, grid)</span><br><span class="line"></span><br><span class="line">x_offset = torch.FloatTensor(a).view(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line">y_offset = torch.FloatTensor(b).view(<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    x_offset = x_offset.cuda()</span><br><span class="line">    y_offset = y_offset.cuda()</span><br><span class="line"></span><br><span class="line">x_y_offset = torch.cat((x_offset, y_offset), <span class="number">1</span>).repeat(<span class="number">1</span>,num_anchors).view(<span class="number">-1</span>,<span class="number">2</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">prediction[:,:,:<span class="number">2</span>] += x_y_offset</span><br></pre></td></tr></table></figure></p>
<p>Apply the anchors to the dimensions of the bounding box.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#log space transform height and the width</span></span><br><span class="line">anchors = torch.FloatTensor(anchors)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    anchors = anchors.cuda()</span><br><span class="line"></span><br><span class="line">anchors = anchors.repeat(grid_size*grid_size, <span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">prediction[:,:,<span class="number">2</span>:<span class="number">4</span>] = torch.exp(prediction[:,:,<span class="number">2</span>:<span class="number">4</span>])*anchors</span><br></pre></td></tr></table></figure></p>
<p>Apply sigmoid activation to the the class scores.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction[:,:,<span class="number">5</span>: <span class="number">5</span> + num_classes] = torch.sigmoid((prediction[:,:, <span class="number">5</span> : <span class="number">5</span> + num_classes]))</span><br></pre></td></tr></table></figure></p>
<p>The last thing we want to do here, is to resize the detections map to the size of the input image. The bounding box attributes here are sized according to the feature map (say, 13 x 13). If the input image was 416 x 416, we multiply the attributes by 32, or the <code>stride</code> variable.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prediction[:,:,:<span class="number">4</span>] *= stride</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> prediction</span><br></pre></td></tr></table></figure></p>
<h2 id="Detection-Layer-Revisited"><a href="#Detection-Layer-Revisited" class="headerlink" title="Detection Layer Revisited"></a>Detection Layer Revisited</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> module_type == <span class="string">'yolo'</span>:        </span><br><span class="line">    anchors = self.module_list[i][<span class="number">0</span>].anchors</span><br><span class="line">    <span class="comment">#Get the input dimensions</span></span><br><span class="line">    inp_dim = int (self.net_info[<span class="string">"height"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Get the number of classes</span></span><br><span class="line">    num_classes = int (module[<span class="string">"classes"</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Transform </span></span><br><span class="line">    x = x.data</span><br><span class="line">    x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> write:              <span class="comment">#if no collector has been intialised. </span></span><br><span class="line">        detections = x</span><br><span class="line">        write = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:       </span><br><span class="line">        detections = torch.cat((detections, x), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">outputs[i] = x</span><br><span class="line"></span><br><span class="line">rturn detections</span><br></pre></td></tr></table></figure>
<h1 id="3-Confidence-Thresholding-and-Non-maximum-Suppression"><a href="#3-Confidence-Thresholding-and-Non-maximum-Suppression" class="headerlink" title="3. Confidence Thresholding and Non-maximum Suppression"></a>3. Confidence Thresholding and Non-maximum Suppression</h1><p>To be precise, our output is a tensor of shape <code>B x 10647 x 85</code>. B is the number of images in a batch, 10647 is the number of bounding boxes predicted per image, and 85 is the number of bounding box attributes.</p>
<p>However,  we must subject our output to objectness score thresholding and Non-maximal suppression, to obtain what we will call in the rest of this post as the<em>true</em> detections. To do that, we will create a function called <code>write_results</code>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_results</span><span class="params">(prediction, confidence, num_classes, nms_conf = <span class="number">0.4</span>)</span>:</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Object-Confidence-Thresholding"><a href="#Object-Confidence-Thresholding" class="headerlink" title="Object Confidence Thresholding"></a>Object Confidence Thresholding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf_mask = (prediction[:,:,<span class="number">4</span>] &gt; confidence).float().unsqueeze(<span class="number">2</span>)</span><br><span class="line">prediction = prediction*conf_mask</span><br></pre></td></tr></table></figure>
<h2 id="Performing-Non-maximum-Suppression"><a href="#Performing-Non-maximum-Suppression" class="headerlink" title="Performing Non-maximum Suppression"></a>Performing Non-maximum Suppression</h2><p>Transform the (center x, center y, height, width) attributes of our boxes, to (top-left corner x, top-left corner y, right-bottom corner x, right-bottom corner y).<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">box_corner = prediction.new(prediction.shape)</span><br><span class="line">box_corner[:,:,<span class="number">0</span>] = (prediction[:,:,<span class="number">0</span>] - prediction[:,:,<span class="number">2</span>]/<span class="number">2</span>)</span><br><span class="line">box_corner[:,:,<span class="number">1</span>] = (prediction[:,:,<span class="number">1</span>] - prediction[:,:,<span class="number">3</span>]/<span class="number">2</span>)</span><br><span class="line">box_corner[:,:,<span class="number">2</span>] = (prediction[:,:,<span class="number">0</span>] + prediction[:,:,<span class="number">2</span>]/<span class="number">2</span>) </span><br><span class="line">box_corner[:,:,<span class="number">3</span>] = (prediction[:,:,<span class="number">1</span>] + prediction[:,:,<span class="number">3</span>]/<span class="number">2</span>)</span><br><span class="line">prediction[:,:,:<span class="number">4</span>] = box_corner[:,:,:<span class="number">4</span>]</span><br></pre></td></tr></table></figure></p>
<p>Confidence thresholding and NMS has to be done for one image at once. This means, we must loop over the first dimension of <code>prediction</code>.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size = prediction.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">write = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ind <span class="keyword">in</span> range(batch_size):</span><br><span class="line">    image_pred = prediction[ind]          <span class="comment">#image Tensor</span></span><br><span class="line">       <span class="comment">#confidence threshholding </span></span><br><span class="line">       <span class="comment">#NMS</span></span><br></pre></td></tr></table></figure></p>
<p>Notice each bounding box row has 85 attributes, out of which 80 are the class scores. At this point, we’re only concerned with the class score having the maximum value. So, we remove the 80 class scores from each row, and instead add the index of the class having the maximum values, as well the class score of that class.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">max_conf, max_conf_score = torch.max(image_pred[:,<span class="number">5</span>:<span class="number">5</span>+ num_classes], <span class="number">1</span>)</span><br><span class="line">max_conf = max_conf.float().unsqueeze(<span class="number">1</span>)</span><br><span class="line">max_conf_score = max_conf_score.float().unsqueeze(<span class="number">1</span>)</span><br><span class="line">seq = (image_pred[:,:<span class="number">5</span>], max_conf, max_conf_score)</span><br><span class="line">image_pred = torch.cat(seq, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<p>Get rid of  the bounding box rows having a object confidence less than the threshold.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">non_zero_ind =  (torch.nonzero(image_pred[:,<span class="number">4</span>]))</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(<span class="number">-1</span>,<span class="number">7</span>)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#For PyTorch 0.4 compatibility</span></span><br><span class="line"><span class="comment">#Since the above code with not raise exception for no detection </span></span><br><span class="line"><span class="comment">#as scalars are supported in PyTorch 0.4</span></span><br><span class="line"><span class="keyword">if</span> image_pred_.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">continue</span></span><br></pre></td></tr></table></figure></p>
<p>The try-except block is there to handle situations where we get no detections. In that case, we use <code>continue</code> to skip the rest of the loop body for this image.</p>
<p>Now, let’s get the classes detected in a an image.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Get the various classes detected in the image</span></span><br><span class="line">img_classes = unique(image_pred_[:,<span class="number">-1</span>]) <span class="comment"># -1 index holds the class index</span></span><br></pre></td></tr></table></figure></p>
<p>Since there can be multiple true detections of the same class, we use a function called <code>unique</code> to get classes present in any given image.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unique</span><span class="params">(tensor)</span>:</span></span><br><span class="line">    tensor_np = tensor.cpu().numpy()</span><br><span class="line">    unique_np = np.unique(tensor_np)</span><br><span class="line">    unique_tensor = torch.from_numpy(unique_np)</span><br><span class="line">    </span><br><span class="line">    tensor_res = tensor.new(unique_tensor.shape)</span><br><span class="line">    tensor_res.copy_(unique_tensor)</span><br><span class="line">    <span class="keyword">return</span> tensor_res</span><br></pre></td></tr></table></figure></p>
<p>Then, we perform NMS classwise.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> cls <span class="keyword">in</span> img_classes:</span><br><span class="line">    <span class="comment">#perform NMS</span></span><br></pre></td></tr></table></figure></p>
<p>Once we are inside the loop, the first thing we do is extract the detections of a particular class (denoted by variable <code>cls</code>).<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#get the detections with one particular class</span></span><br><span class="line">cls_mask = image_pred_*(image_pred_[:,<span class="number">-1</span>] == cls).float().unsqueeze(<span class="number">1</span>)</span><br><span class="line">class_mask_ind = torch.nonzero(cls_mask[:,<span class="number">-2</span>]).squeeze()</span><br><span class="line">image_pred_class = image_pred_[class_mask_ind].view(<span class="number">-1</span>,<span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#sort the detections such that the entry with the maximum objectness confidence is at the top</span></span><br><span class="line">conf_sort_index = torch.sort(image_pred_class[:,<span class="number">4</span>], descending = <span class="keyword">True</span> )[<span class="number">1</span>]</span><br><span class="line">image_pred_class = image_pred_class[conf_sort_index]</span><br><span class="line">idx = image_pred_class.size(<span class="number">0</span>)   <span class="comment">#Number of detections</span></span><br></pre></td></tr></table></figure></p>
<p>Now, we perform NMS.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(idx):</span><br><span class="line">    <span class="comment">#Get the IOUs of all boxes that come after the one we are looking at </span></span><br><span class="line">    <span class="comment">#in the loop</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        ious = bbox_iou(image_pred_class[i].unsqueeze(<span class="number">0</span>), image_pred_class[i+<span class="number">1</span>:])</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> IndexError:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#Zero out all the detections that have IoU &gt; treshhold</span></span><br><span class="line">    iou_mask = (ious &lt; nms_conf).float().unsqueeze(<span class="number">1</span>)</span><br><span class="line">    image_pred_class[i+<span class="number">1</span>:] *= iou_mask       </span><br><span class="line"></span><br><span class="line">    <span class="comment">#Remove the non-zero entries</span></span><br><span class="line">    non_zero_ind = torch.nonzero(image_pred_class[:,<span class="number">4</span>]).squeeze()</span><br><span class="line">    image_pred_class = image_pred_class[non_zero_ind].view(<span class="number">-1</span>,<span class="number">7</span>)</span><br></pre></td></tr></table></figure></p>
<p>Here, we use a function <code>bbox_iou</code>. The first input is the bounding box row that is indexed by the the variable <code>i</code> in the loop.<br>Second input is a tensor of multiple rows of bounding boxes. The output of the function <code>bbox_iou</code> is a tensor containing IoUs of the bounding box represented by the first input with each of the bounding boxes present in the second input.<br>If we have two bounding boxes of the same class having an IoU larger than a threshold, then the one with lower class confidence is eliminated. </p>
<p>Also notice, we have put the line of code to compute the ious in a <code>try-catch</code> block. This is because the loop is designed to run <code>idx</code> iterations. However, as we proceed with the loop, a number of bounding boxes may be removed from <code>image_pred_class</code>. This means, we cannot have idx iterations in most instances. Hence, we might try to index a value that is out of bounds (<code>IndexError</code>), or the slice <code>image_pred_class[i+1:]</code> may return an empty tensor, assigning which triggers a <code>ValueError</code>. At that point, we can ascertain that NMS can remove no further bounding boxes, and we break out of the loop.</p>
<p><strong>Calculating the IoU</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_iou</span><span class="params">(box1, box2)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns the IoU of two bounding boxes </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#Get the coordinates of bounding boxes</span></span><br><span class="line">    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,<span class="number">0</span>], box1[:,<span class="number">1</span>], box1[:,<span class="number">2</span>], box1[:,<span class="number">3</span>]</span><br><span class="line">    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,<span class="number">0</span>], box2[:,<span class="number">1</span>], box2[:,<span class="number">2</span>], box2[:,<span class="number">3</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#get the corrdinates of the intersection rectangle</span></span><br><span class="line">    inter_rect_x1 =  torch.max(b1_x1, b2_x1)</span><br><span class="line">    inter_rect_y1 =  torch.max(b1_y1, b2_y1)</span><br><span class="line">    inter_rect_x2 =  torch.min(b1_x2, b2_x2)</span><br><span class="line">    inter_rect_y2 =  torch.min(b1_y2, b2_y2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Intersection area</span></span><br><span class="line">    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + <span class="number">1</span>, min=<span class="number">0</span>) * torch.clamp(inter_rect_y2 - inter_rect_y1 + <span class="number">1</span>, min=<span class="number">0</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">#Union Area</span></span><br><span class="line">    b1_area = (b1_x2 - b1_x1 + <span class="number">1</span>)*(b1_y2 - b1_y1 + <span class="number">1</span>)</span><br><span class="line">    b2_area = (b2_x2 - b2_x1 + <span class="number">1</span>)*(b2_y2 - b2_y1 + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    iou = inter_area / (b1_area + b2_area - inter_area)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure></p>
<h2 id="Writing-the-predictions"><a href="#Writing-the-predictions" class="headerlink" title="Writing the predictions"></a>Writing the predictions</h2><p>The function <code>write_results</code> outputs a tensor of shape D x 8. Here D is the true detections in all of images, each represented by a row. Each detections has 8 attributes, namely, <strong>index of the image in the batch to which the detection belongs to, 4 corner coordinates, objectness score, the score of class with maximum confidence, and the index of that class.</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">batch_ind = image_pred_class.new(image_pred_class.size(<span class="number">0</span>), <span class="number">1</span>).fill_(ind)      </span><br><span class="line"><span class="comment">#Repeat the batch_id for as many detections of the class cls in the image</span></span><br><span class="line">seq = batch_ind, image_pred_class</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> write:</span><br><span class="line">    output = torch.cat(seq,<span class="number">1</span>)</span><br><span class="line">    write = <span class="keyword">True</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    out = torch.cat(seq,<span class="number">1</span>)</span><br><span class="line">    output = torch.cat((output,out))</span><br></pre></td></tr></table></figure></p>
<h1 id="4-Designing-the-input-and-the-output-pipelines"><a href="#4-Designing-the-input-and-the-output-pipelines" class="headerlink" title="4. Designing the input and the output pipelines"></a>4. Designing the input and the output pipelines</h1><h2 id="Loading-the-Network"><a href="#Loading-the-Network" class="headerlink" title="Loading the Network"></a>Loading the Network</h2><p>Load the class file.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">80</span>    <span class="comment">#For COCO</span></span><br><span class="line">classes = load_classes(<span class="string">"data/coco.names"</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_classes</span><span class="params">(namesfile)</span>:</span></span><br><span class="line">    fp = open(namesfile, <span class="string">"r"</span>)</span><br><span class="line">    names = fp.read().split(<span class="string">"\n"</span>)[:<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> names</span><br></pre></td></tr></table></figure>
<p>Initialize the network and load weights.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Set up the neural network</span></span><br><span class="line">print(<span class="string">"Loading network....."</span>)</span><br><span class="line">model = Darknet(args.cfgfile)</span><br><span class="line">model.load_weights(args.weightsfile)</span><br><span class="line">print(<span class="string">"Network successfully loaded"</span>)</span><br><span class="line"></span><br><span class="line">model.net_info[<span class="string">"height"</span>] = args.reso</span><br><span class="line">inp_dim = int(model.net_info[<span class="string">"height"</span>])</span><br><span class="line"><span class="keyword">assert</span> inp_dim % <span class="number">32</span> == <span class="number">0</span> </span><br><span class="line"><span class="keyword">assert</span> inp_dim &gt; <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#If there's a GPU availible, put the model on GPU</span></span><br><span class="line"><span class="keyword">if</span> CUDA:</span><br><span class="line">    model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment">#Set the model in evaluation mode</span></span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure></p>
<h2 id="Read-the-Input-images"><a href="#Read-the-Input-images" class="headerlink" title="Read the Input images"></a>Read the Input images</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">read_dir = time.time()</span><br><span class="line"><span class="comment">#Detection phase</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    imlist = [osp.join(osp.realpath(<span class="string">'.'</span>), images, img) <span class="keyword">for</span> img <span class="keyword">in</span> os.listdir(images)]</span><br><span class="line"><span class="keyword">except</span> NotADirectoryError:</span><br><span class="line">    imlist = []</span><br><span class="line">    imlist.append(osp.join(osp.realpath(<span class="string">'.'</span>), images))</span><br><span class="line"><span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"No file or directory with the name &#123;&#125;"</span>.format(images))</span><br><span class="line">    exit()</span><br></pre></td></tr></table></figure>
<p>use OpenCV to load the images.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load_batch = time.time()</span><br><span class="line">loaded_ims = [cv2.imread(x) <span class="keyword">for</span> x <span class="keyword">in</span> imlist]</span><br></pre></td></tr></table></figure></p>
<p>Write the function <code>letterbox_image</code> to resizes our image, keeping the aspect ratio consistent, and padding the left out areas with the color (128,128,128).<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">letterbox_image</span><span class="params">(img, inp_dim)</span>:</span></span><br><span class="line">    <span class="string">'''resize image with unchanged aspect ratio using padding'''</span></span><br><span class="line">    img_w, img_h = img.shape[<span class="number">1</span>], img.shape[<span class="number">0</span>]</span><br><span class="line">    w, h = inp_dim</span><br><span class="line">    new_w = int(img_w * min(w/img_w, h/img_h))</span><br><span class="line">    new_h = int(img_h * min(w/img_w, h/img_h))</span><br><span class="line">    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)</span><br><span class="line">    </span><br><span class="line">    canvas = np.full((inp_dim[<span class="number">1</span>], inp_dim[<span class="number">0</span>], <span class="number">3</span>), <span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">    canvas[(h-new_h)//<span class="number">2</span>:(h-new_h)//<span class="number">2</span> + new_h,(w-new_w)//<span class="number">2</span>:(w-new_w)//<span class="number">2</span> + new_w,  :] = resized_image</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> canvas</span><br></pre></td></tr></table></figure></p>
<p>Write the function <code>prep_image</code> to takes a OpenCV images and converts it to PyTorch’s input format.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prep_image</span><span class="params">(img, inp_dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Prepare image for inputting to the neural network. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns a Variable </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    img = cv2.resize(img, (inp_dim, inp_dim))</span><br><span class="line">    img = img[:,:,::<span class="number">-1</span>].transpose((<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)).copy()</span><br><span class="line">    img = torch.from_numpy(img).float().div(<span class="number">255.0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          
            <a href="/tags/Computer-Vision/" rel="tag"># Computer Vision</a>
          
            <a href="/tags/Object-Detection/" rel="tag"># Object Detection</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/16/google_deeplab代码分析/" rel="next" title="google_deeplab代码分析">
                <i class="fa fa-chevron-left"></i> google_deeplab代码分析
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/16/something_about_python/" rel="prev" title="Something about Python">
                Something about Python <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Pan Sicheng</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Creating-the-layers-of-the-network-architecture"><span class="nav-number">1.</span> <span class="nav-text">1. Creating the layers of the network architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Configuration-File"><span class="nav-number">1.1.</span> <span class="nav-text">Configuration File</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parsing-the-configuration-file"><span class="nav-number">1.2.</span> <span class="nav-text">Parsing the configuration file</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Creating-the-building-blocks"><span class="nav-number">1.3.</span> <span class="nav-text">Creating the building blocks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-Implementing-the-the-forward-pass-of-the-network"><span class="nav-number">2.</span> <span class="nav-text">2. Implementing the the forward pass of the network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Defining-the-Network"><span class="nav-number">2.1.</span> <span class="nav-text">Defining the Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementing-the-forward-pass-of-the-network"><span class="nav-number">2.2.</span> <span class="nav-text">Implementing the forward pass of the network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Convolutional-and-Upsample-Layers"><span class="nav-number">2.3.</span> <span class="nav-text">Convolutional and Upsample Layers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Route-Layer-Shortcut-Layer"><span class="nav-number">2.4.</span> <span class="nav-text">Route Layer / Shortcut Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLO-Detection-Layer"><span class="nav-number">2.5.</span> <span class="nav-text">YOLO (Detection Layer)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transforming-the-output"><span class="nav-number">2.6.</span> <span class="nav-text">Transforming the output</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Detection-Layer-Revisited"><span class="nav-number">2.7.</span> <span class="nav-text">Detection Layer Revisited</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Confidence-Thresholding-and-Non-maximum-Suppression"><span class="nav-number">3.</span> <span class="nav-text">3. Confidence Thresholding and Non-maximum Suppression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Object-Confidence-Thresholding"><span class="nav-number">3.1.</span> <span class="nav-text">Object Confidence Thresholding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performing-Non-maximum-Suppression"><span class="nav-number">3.2.</span> <span class="nav-text">Performing Non-maximum Suppression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Writing-the-predictions"><span class="nav-number">3.3.</span> <span class="nav-text">Writing the predictions</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Designing-the-input-and-the-output-pipelines"><span class="nav-number">4.</span> <span class="nav-text">4. Designing the input and the output pipelines</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Loading-the-Network"><span class="nav-number">4.1.</span> <span class="nav-text">Loading the Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Read-the-Input-images"><span class="nav-number">4.2.</span> <span class="nav-text">Read the Input images</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Pan Sicheng</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
